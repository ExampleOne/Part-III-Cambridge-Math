\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\usepackage{gensymb}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{adjustbox}
\usepackage{physics}
\usepackage{dsfont}
\usepackage{cancel}

\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{claim}{Claim}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newcommand{\id}{\text{id}}

\title{Quantum Information Theory}
\author{quinten tupker}
\date{January 22 2021 - \today}

\begin{document}

\maketitle

\section*{Introduction}

These notes are based on the course lectured by Professor S Strelchuk in Lent 2020.
This was lectured online due to measures taken to counter the spread of Covid-19
in the UK. These are not necessarily an accurate representation of what was
lectures, and represent solely my personal notes on the content of the course,
combinged with probably, very very many personal notes and digressions... Of
course, any corrections/comments would be appreciated.

Information theory is the theory of information storage and transmission. It
provides the theoretical limits on what is possible with information
technologies in much of our world, and a framework to study many other fields,
such as animal communication as well. This is the quantum qersion of that
theory.

\section{Classical Information Theory}

We begin by observing that information is closely related to uncertainty. In
particular, what might be able to say it is the opposite of uncertainty, and so
then, it is no surprise that we build our theory of information using concepts
from probability theory. As such we define

\begin{definition}
  The \textbf{surprisal} of random variable $X$ taking values in discrete finite
  \textbf{alphabet} $J$ according to distribution $p(x)$ is
  $$ \mathcal{I}(x) = - \log(p(x)) $$
\end{definition}

\begin{definition}
  The \textbf{Shannon entropy} of a random variable $X$ is
  $$ H(X) = -\sum_{x \in J} p(x) \log(p(x)) $$
  (the logarithm is base 2)
\end{definition}

This may appear to be a somewhat arbitrary definition, but it has a strong
theoretical basis given that

\begin{theorem}
  The \textbf{Shannon Source Coding Theorem} states (informally) that the limit
  that information can be compressed so that it can be reliably retrieved is the
  Shannon entropy of the source.
\end{theorem}

Here a basic example of a source is a \textbf{memoryless source} which is an
object producing a sequence of signals, but since it is memoryless, each signal
is completely independent from any other, so $\mathbb{P}(u_1, \dots, u_n) =
\mathbb{P}(u_1) \dots \mathbb{P}(u_n)$. It is also known as an \textbf{i.i.d.
  information source}.

But how do we actually compress information? Conceptually there are two ways

\begin{itemize}
\item a \textbf{variable length encoding} stores higher probability signals in
  shorter codes, and lower probability signals in longer codes.
\item a \textbf{fixed length encoding} stores higher probability signals in
  unique fixed length codes, and lower probability signals in the same fixed
  code. 
\end{itemize}

\begin{example}
  An example of a fixed length code for the numbers $1, \dots, 8$ are their
  binary representations, but if we also know that $p(1, \dots, 8) = 1/2, 1/4,
  1/8, 1/16, 1/64, 1/64, 1/64, 1/64$ then the code $C(1, \dots, 8) = 0, 10, 110,
  1110, 111100, 111101, 111110, 111111$ has an average length 2 compared to the
  fixed length of 3. Furthermore, the Shannon entropy of this source is 2 as
  well, so this is maximally efficient.
\end{example}

\subsection{Classical Data Compression}

Let's start making the definitions necessary to formalise compression.

\begin{definition}
  A \textbf{compression map} is a map $C^n : u^{(n)} = (u_1, \dots, u_n) \mapsto
  x(x_1, \dots, x_{nR})$ sending a \textbf{message} $u$ to a \textbf{code} $x$.
\end{definition}

\begin{definition}
  A \textbf{decompression map} $D^n$ sends $D^n : x \in \{0, 1\}^{\lceil n R
    \rceil} \mapsto u'^{(n)}$ with probability $\mathbb{P}(u^{(n)} | x)$.
\end{definition}

\begin{definition}
  A \textbf{code} of rate $R$ and blocklength $n$ is the triple $(C^n, D^n, R)$.
\end{definition}

Here we can compute the \textbf{probability of error} as

\begin{equation}
  P^{(n)}_{av}(C_n) = \sum_{u^{(n)} \in J^n} \mathbb{P}(u^{(n)}) \mathbb{P}(D^n
  (C^n(u^{(n)})) \neq u^{(n)}).
\end{equation}

[End of lecture 1]

\begin{definition}
  A compression/decompression scheme is \textbf{reliable} iff $\forall \epsilon
  > 0 \exists $ sequence of codes $C_n$ such that $\lim_{n \to \infty}
  P_{av}^{(n)} (C_n) = 0$
\end{definition}
(does this require the code to i.i.d?) As such one could define the data
compression rate as

\begin{equation}
  \inf \{ R : \exists C_n = \{C^n, D^n, R\} : \lim_{n \to \infty} P_{av}^{(n)}(C_n) = 0 \}
\end{equation}

\subsection{Typical Sequences}

It is honestly surprising how easy it is to prove Shannon's source coding
theorem, since we find we in fact only need one simple, and honestly rather
crude tool to do so. That tool is

\begin{definition}
  a \textbf{typical set}, denoted $T_\epsilon^{(n)}$, which is the set of
  sequences $u = (u_1, \dots, u_n)$ satisfying
  \begin{equation}
    2^{-n (H(u) + \epsilon)} \leq \mathbb{P}(u) \leq 2^{-n(H(u) - \epsilon)}
  \end{equation}
\end{definition}

Why ``typical''? Because if we have a memoryless source generating a sequence of
length $n$, then a ``typical'' sequence occurs with probability

\begin{equation}
  \prod_{u \in J} \mathbb{P}(u)^{n \mathbb{P}(u)} = 2^{-n H(u)}
\end{equation}
(a typical sequence would have the expected value as the number of occurences of
each letter in the alphabet). We consequently find the following theorem (not
proven)

\begin{theorem}
  $\forall \epsilon, \delta > 0, \exists n$ such that
  \begin{enumerate}
    \item $u \in T_\epsilon^{(n)} \implies H(u) - \epsilon \leq \frac{-1}{n}
      \log(\mathbb{P}(u)) \leq H(u) + \epsilon$
    \item $\mathbb{P}(T_\epsilon^{(n)}) > 1 - \delta$
    \item $|T_\epsilon^{(n)}| \leq 2^{n(H(u) + \epsilon)}$
    \item $|T_\epsilon^{(n)} > (1 - \delta) 2^{2(H(u) - \epsilon)}$
  \end{enumerate}
\end{theorem}
  
and so

\begin{corollary}
  $\forall \epsilon, \delta > 0, \exists n_0, \forall n > n_0, J^n$ decomposes
  into the disjoint \textbf{atypical and typical sets} $A_\epsilon^{(n)},
  T_\epsilon^{(n)}$ satisfying
  \begin{enumerate}
    \item $\mathbb{P}(A_\epsilon^{(n)}) < \delta$
    \item $2^{-n(H(u) + \epsilon)} \leq \mathbb{P}(u) \leq 2^{-n(H(n) -
        \epsilon)}$ (on the typical set?)
  \end{enumerate}
\end{corollary}

That allows us to formally state

\begin{theorem}
  \textbf{Shannon's source coding theorem}, which claims that for a i.i.d.
  source $U$, if $R > H(U)$, then we can find a reliable compression scheme and
  if $R < H(U)$ there are no reliable compression schemes.
\end{theorem}

\begin{proof}
  If $R > H(U)$ then pick $\epsilon > 0$ such that $H(U) + \epsilon < R$ and $n$
  such that $T_\epsilon^{(n)}$ satisfies our typical set theorem condiions. Then
  for $\delta > 0$ there are at most $2^{n(H(U) + \epsilon)} < 2^{nR}$
  $\epsilon$-typical sequences. Our compression scheme then works according to
  \begin{enumerate}
  \item Split $J^n$ into typical and atypical sequences
  \item order typical sequences somehow (say lexicographically), assigning
    each sequence a binary index.
  \item typical sequences are sent to their binary code, prefixed with a 1,
    leading to a total length of $\lceil nR \rceil + 1$
  \item atypical sequences are all sent to the fixed string $00 \dots 0$ of
    length $\lceil nR \rceil + 1$
  \end{enumerate}

  If $R < H(U)$ there does not exist a reliable compression scheme (see lemma
  below).
\end{proof}

\begin{lemma}
  For a collection of strings of length $n$, $S(n)$, with $|S(n)| \leq 2^{nR}, R
  < H(U)$.
  Then, $\forall \delta > 0, \exists n, \sum_{u \in S(n)} \mathbb{P}(u) <
  \delta$
\end{lemma}

Basically, when $R < H(U)$ we will always get some typical sets that are
indistinguishable after compression. Assume $S(n)$ is a set of such
indistinguishables. This lemma states that these become less significant? 

\begin{proof}
Again, we split $S(n)$ into its typical and atypical part and then observe
\begin{align*}
\mathbb{P}(S(n)) &= \sum_{u \in S(n)} \mathbb{P}(u) \\
&= \sum_{u \in S(n) \cap T_\epsilon^{(n)}} \mathbb{P}(u) + 
\sum_{u \in S(n) \cap A_\epsilon^{(n)}} \mathbb{P}(u) \\
&\leq |S(n)| 2^{-n(H(n) - \epsilon)} + \mathbb{P}(A_\epsilon^{(n)}) \\
&\leq 2^{-n(H(n) - R)} + \mathbb{A_\epsilon^{(n)}}
\end{align*}
\end{proof}

which completes the proof of Shannon's source coding theorem. Finally, we note
that using a variable length encoding scheme or something like that does not
fundamentally change this result. [End of lecture 2]

Today we discuss the types of relationships one can have between various types
of entropy. As such,

\begin{definition}
  the \textbf{joint entropy of $X, Y$} is
  \begin{equation}
    H(X, Y) = - \sum_{x \in J_X} \sum_{y \in J_Y} p(x, y) \log(p(x, y))
  \end{equation}
\end{definition}

\begin{definition}
  the \textbf{conditional entropy} is
  \begin{equation}
    H(Y | X) = - \sum_{x \in J_X} \sum_{y \in J_Y} p(x, y) \log(p(y | x))
    = - \sum_{x \in J_X} p(x) H(Y | X = x)
  \end{equation}
\end{definition}

where we note the ``chain rule''

\begin{equation}
  H(X, Y) = H(Y | X) + H(X)
\end{equation}

which generalises nicely as

\begin{equation}
  H(X, Y, Z) = H(X) + H(Y | X) + H(Z | Y, X)
\end{equation}

and similar for higher numbers of variables.

\begin{definition}
  We further define $p$ to be \textbf{absolutely continuous} wrt $q$ iff $q(x) =
  0 \implies p(x) = 0$ or equivalently $\text{supp}(p) \subseteq \text{supp}(q)$
  which we can denote as $p << q$.
\end{definition}

and

\begin{definition}
  the \textbf{relative entropy} or \textbf{Kullback-Leibler divergence} of $p <<
  q$ to be
  \begin{equation}
    D(p || q) = \sum_{x \in J} p(x) \log(p(x) / q(x))
  \end{equation}
  Note that if $q(x) = 1 \forall x \in J$ then $D(p || q) = -H(X)$ so this is
  stronger than the Shannon entropy.
\end{definition}

This measures how different two distributions are in a certain sense, however,
it certainly is not a metric (not symmetric, and no triangle inequality).

\begin{definition}
We similarly define the \textbf{mutual information} between $X, Y$ to be
\begin{equation}
I(X : Y) = H(X) + H(Y) - H(X, Y) = H(X) - H(X | Y)
\end{equation}
\end{definition}

\begin{definition}
and the \textbf{conditional mutual information} (CMI)
\begin{equation}
I(X : Y | Z) = H(X | Z) - H(X | Y, Z)
\end{equation}
\end{definition}

Intuition wise, one can think of $H(X, Y)$ as adding entropy, $I(X:Y)$ as taking
the intersection between the two, $D(p || q)$ as measuring the difference
between one distribution contained in another, and $H(Y|X)$ as removing the
overlap of one distribution into another ($H(Y | X) = H(Y) - I(Y : X)$). This
also explains the commutativeness of these various operations. I'm not sure
about the CMI yet...

From here we can state

\begin{theorem}
the data processing inequality for Markov Chain $X \to Y \to Z$:
\begin{equation}
I(X : Y) \geq I(X : Z).
\end{equation}
\end{theorem}

If we imagine $X$ as a perfect source, $Y$ as an observed, noisy signal, and $Z$
as a ``cleaned up'' version of $Y$ after some data processing, then this states
that no matter the data processing used to clean up $Y$, $Z$ can never contain
more information about $X$ than $Y$.

We finally have the following theorem.

\begin{theorem}
\begin{enumerate}
\item $p << q \implies D(p || q) \geq 0$ with equality iff $p = q$
\item $H(x) \geq 0$ with equality iff $X$ deterministic
\item $H(X | Y) \geq 0$ or equivalently $H(X, Y) \geq H(Y)$
\item $H(X) \leq \log(|J|)$
\item $H(X, Y) \leq H(X) + H(Y)$ or equivalently $H(Y) \geq H(Y | X)$ with
equality iff $X \perp Y$
\item $H$ is concave meaning $H(\lambda p_x + (1 - \lambda) p_y) \geq
 \lambda H(p_x) + (1 - \lambda) H(p_Y)$
\item $I(X : Y) \geq 0$ and equal iff $X \perp Y$
\end{enumerate}
\end{theorem}

All of these are exercises on the example sheet. [End of lecture 4]

\subsection{Classical Information Transmission}

Let's build the infrastructure for a bound on reliable information transmission.

\begin{definition}
  A \textbf{discrete channel} is a combination of
  \begin{itemize}
  \item discrete alphabets $J_X, J_Y$
  \item a set of conditional probabilities $\mathbb{P}(y_1, \dots, y_n | x_1,
    \dots x_m)$
  \end{itemize}
\end{definition}

\begin{definition}
  A memoryless channel is one satisfying
  \begin{equation}
    \mathbb{P}(y_1, \dots, y_n | x_1, \dots, x_n) = \prod_{i = 1}^n \mathbb{P}(y_i | x_i)
  \end{equation}
  These are completely characterised by the so-called \textbf{channel matrix} $P =
  \mathbb{P}(y | x)$.
\end{definition}

\begin{example}
  For example we can consider a \textbf{binary symmetric channel} for
  $\mathbb{P}(0 | 0) = \mathbb{P}(1 | 1) = 1 - p$ where $p_{err} = 3p^2(1 - p) +
  p^3 = 3p^2 - 2p^3$ if we use the redundant codingg $0 \mapsto 000, 1 \mapsto
  111$.
\end{example}

\begin{definition}
  Now for messages $m=x_1, \dots, x_n$ and $m'=y_1, \dots, y_m$ and $m \in [M] =
  \{1, \dots, M\}$ we \textbf{encoding} $E_n : [M] \to J_X^n$, \textbf{decoder}
  $D_n : [M] \to J_X^n$, and rate of encoding $R$ such that $M = \lfloor 2^{nR}
  \rfloor$. The triple $C_n = (E_n, D_n, R)$ is then called an \textbf{error
    correcting code}.
\end{definition}

\begin{definition}
  and unlike the average error probabilities consider in information storage,
  here the error probability is the maximum
  \begin{equation}
    P_{err}(C_n) = \max_{m \in [M]} \mathbb{P}(D_n(Y^{(n)}) \neq m | X^{(n)} = E_n(m))
  \end{equation}
\end{definition}

\begin{definition}
  and rate $R$ is called \textbf{achievable} if there exists an error correcting
  code such that
  \begin{equation}
    \lim_{n \to \infty} P_{err}(C_n) = 0
  \end{equation}
\end{definition}

and so quite naturally

\begin{definition}
the \textbf{capacity} of a discrete memoryless channel is $C(N) = \sup \{R : R
\text{ is achievable}\}$.
\end{definition}

for which

\begin{theorem}
\textbf{Shannon's noisy channel coding theorem} states that
\begin{equation}
C(N) = \max_{p(x)} I(X : Y)
\end{equation}
(note that the maximum is only taken over the input, not the output
distribution)
\end{theorem}

\begin{theorem}
which has the properties
\begin{itemize}
\item $C(N) \geq 0$
\item $C(X) \leq \log |J_X|, \log |J_Y|$
\end{itemize}
\end{theorem}

rigorous proof of which is found in the 1991 book \textit{Elements of
  Information Theory}.

\begin{example}
Finally, in the binary symmetric channel from before we find that
\begin{equation}
I(X : Y) = H(Y) - H(Y | X) = H(Y) - h(p) \implies C(N) = 1 - h(p)
\end{equation}
[End of lecture 4]
\end{example}

\section{Quantum Information Theory}

\subsection{Quantum States}

The lecturer reviews quantum states and how they are represented using linear
algebra. Here we note that the \textbf{Hemming space} is the space
$\mathbb{C}^{2^n}$, and use $B(H)$ to represent the space of bounded linear
operators on $H$. We also mention the \textbf{Pauli Matrices} and their
properties

\begin{itemize}
\item $\sigma^2_\alpha = I$
\item $\sigma_\alpha \sigma_\beta = i\epsilon_{\alpha \beta \gamma}
  \sigma_\gamma$
\item $\{\sigma_\alpha, \sigma_\beta\} = 0$ for $\alpha \neq \beta$
\end{itemize}

\subsection{Open Quantum Systems}

We generalise our formalism a bit to account for external noise. As such, the
lecturer reviews the postulates of quantum mechanics. Then we define

\begin{definition}
  a \textbf{density matrix} to be a matrix representing a combination states
  \begin{equation}
    \rho = \sum p_i \ket{\psi_i} \bra{\psi_i}
  \end{equation}
\end{definition}

which can be equivalently be defined by the properties that

\begin{itemize}
\item $\rho \geq 0$ (positive semi-definite) which implies $\rho$ Hermitian
\item $\tr \rho = 1$
\end{itemize}

which serves as a more abstract definition density matrices. If we let
$\mathcal{D}(H)$ represent the \textbf{set of density matrices} over space $H$
then we notice the property that $\sigma_i \in \mathcal{D}(H), \sum p_i = 1
\implies \sum p_i \sigma_i \in \mathcal{D}(H).$ [End of lecture 5] 

Introducing some vocabulary, we say 

\begin{definition}
a state $\rho$ is a \textbf{pure state} if $\rho = \ket{\psi} \bra{psi}$ or
equivalently that $\rho^2 = \rho$ or $\tr \rho^2 = 1$. If not, we call $\rho$ a
\textbf{mixed state}
\end{definition}

\begin{definition}
and we can define the \textbf{purity} of $\rho$ to be $\tr \rho^2$ which is 1
for a pure state and strictly less than 1 for a mixed state. It is minimised for
$I / d$.
\end{definition}

\begin{definition}
  For a product space $H_A \otimes H_B$ we find it useful to define the
  \textbf{partial trace} $\tr_B$ that sends an operator
  \begin{equation}
    X_{AB} \mapsto X_A = \tr_B X_{AB} = \sum \bra{e_i^B} X_{AB} \ket{e_i^B}
  \end{equation}
  for an orthonormal basis $e_i^B$.
\end{definition}

We can then define $\tr_{AB} = \tr_A \circ \tr_B$. For observable $M_{AB} = M_A
\otimes I_B$ we then find that the expectation value
\begin{equation}
\langle M_{AB} \rangle = \tr(M_{AB} \rho_{AB}) = \tr(M_A \rho_A)
\end{equation}
where
\begin{equation}
\rho_{AB} = \sum{i,j,\alpha,\beta} a_{i\alpha, j\beta} \ket{i}\bra{j}_A \otimes
\ket{\alpha}\bra{\beta}_B 
\implies \rho_A = \sum_{i, j, \alpha} a_{i\alpha, j\alpha} \ket{i}\bra{j}
\end{equation}

We then see that $\tr_B(\rho_A \otimes \rho_B) = \rho_A$ for states $\rho_A,
\rho_B$ (called a \textbf{bipartite system}), while for maximally entangled
states such as the \textbf{Bell states} or \textbf{EPR pairs}

\begin{align}
\ket{\phi^\pm} = \frac{1}{\sqrt{2}} (\ket{00} \pm \ket{11}) \\
\ket{\chi^\pm} = \frac{1}{\sqrt{2}} (\ket{01} \pm \ket{10})
\end{align}

have $\tr_B \rho_{AB} = I_A / 2$ meaning that the qubits are maximally mixed,
even though the global state is known! For both density matrices and state
vectors, we define them to be \textbf{separable} if they can be written as
tensor products $\ket{\psi} = \ket{\phi}_A \otimes \ket{\chi}_B$ or $\rho_{AB} =
\sum_i p_i \omega_i^A \otimes \sigma_i^B$ and if not we call them
\textbf{entangled}. [End of lecture 6]

\subsection{Schmidt Decomposition}

The lecturer gives the following special case of the Schmidt decomposition theorem:

\begin{theorem}
	The \textbf{Schmidt decomposition theorem} states that a pure state 
	$\ket{\Psi_{AB}}$ with $d_A = \text{dim}(\mathcal{H}_A), 
	d_B = \text{dim}(\mathcal{H}_B)$ where the spaces 
	$\mathcal{H}_A, \mathcal{A}_B$ has Schmidt bases $\ket{i_A}, \ket{i_B}$ 
	and nonnegative real \textbf{Schmidt coefficients} $\lambda_i$ such that
	\begin{equation}
		\sum_{i = 1}^{\min(d_A, d_B)} \lambda_B^2 = 1
  \end{equation}
  and note here that $\rho_A = \sum_i \lambda_i^2 \ket{i_A} \bra{i_A}$ and 
  the same for $\rho_B$.
\end{theorem}
\begin{proof}
  The lecturer provides a proof using the Singular Value Decomposition (SVD), and uses the fact that $\tr \rho_A = 1 = \tr \rho_B$ to get $\sum \lambda_i^2 = 1$.
\end{proof}

From here we note that if $\rho_A, \rho_B$ have no degenereate eigenvalues 
other than $0$, the  $\Psi_{AB}$ is determined uniquely by $\rho_A, \rho_B$ 
since one can
\begin{enumerate}
  \item diagonalise $\rho_A, \rho_B$
  \item and then match up eigenvectors corresponding to the same eigenvalues
\end{enumerate}

We also define
\begin{definition}
  the texbf{Schmidt rank} of pure bipartite state $\ket{\Psi_{AB}}$ to be the
  non-zero coefficients in its Schmidt decomposition. This is denoted by
  $n(\Psi_{AB})$.
\end{definition}
This provides a simple way of checking whether or not $\ket{\Psi_{AB}}$ is
entangled since it is entangled iff $n(\Psi_{AB}) > 1$.

\subsection{Purification}

A useful mathematical trick is to represent mixed states as pure states. In
particular, for density $\rho_A$ on system $A$ we can introduce a 
\textbf{reference system} $R$ and pure state $\Psi_{AR}$ such that
\begin{equation}
  \rho_A = \tr_R ( \ket{\Psi_{AR}} \bra{\Psi_{AR}} )
\end{equation}
In particular, we can do so for $\mathcal{H}_R \simeq \mathcal{H}_A$ by considering 
for
\begin{equation}
  \rho_A = \sum_i p_i \ket{i_A} \bra{i_A}
\end{equation}
the pure state
\begin{equation}
  \ket{\Psi_{AB}} = \sum_i \sqrt{p_i} \bra{i_A} \otimes \bra{i_R}
\end{equation}
(which is just the Schmidt decomposition of $\Psi_{AR}$. More generally, we can 
use the \textbf{canonical purification}
\begin{equation}
  \ket{\Psi_{AR}} = \sqrt{d} (\sqrt{\rho_A} \otimes I_R ) \ket{\Omega}
\end{equation}
where
\begin{equation}
  \ket{\Omega} = \frac{1}{\sqrt{d}} \sum_{i=1}^d \ket{i}\ket{i} \in
  \mathcal{H}_A \otimes \mathcal{H}_R
\end{equation}

Also, we not that any pure state $\ket{\Psi_{AR}}$ with reduced state $\rho_A$ can
be written as 
\begin{equation}
  \ket{\Psi_{AR}} = \sqrt{d} (\sqrt{\rho_A} \otimes V) \ket{\Omega}
\end{equation}
where $V$ is an isometry rotating $\ket{i}$ into the Schmidt basis of 
$\ket{\Psi_{AR}}$. [End of lecture 7]

\subsection{The no cloning theorem}

Although it shows up in part II, we review the no cloning theorem.

\begin{theorem}
  The \textbf{no cloning theorem} states that there is no quantum
  process that universally clones quantum states. More precisely,
  one can only design a quantum process that clones orthogonal states.
\end{theorem}
\begin{proof}
  Suppose that for two states $\ket{\psi}, \ket{\phi}$ we have that for any
  state $\ket{s}$ 
  \begin{align*}
    U(\ket{\psi} \otimes \ket{s}) &= \ket{\psi} \otimes \ket{\psi} \\
    U(\ket{\phi} \otimes \ket{s}) &= \ket{\phi} \otimes \ket{\phi}
  \end{align*}
  meaning that by taking the inner product we have $\bra{\phi} \ket{\psi}
  = \bra{\phi} \ket{\psi}^2$ meaning that $\phi, \psi$ are either identical
  or orthogonal. Now, one might argue that this assumes that we are in a pure
  state, and also assumes that any operation we use is unitary. However, 
  by considering the purification of a non-pure state this generalises to
  non-pure states. Also, by adding an ancilla we can make (any?) state unitary
  so we this generalises to copying any quantum state.
\end{proof}

\subsection{Time evaluation of open systems}

\subsection{Quantum operations}

To describe changes in open quantum systems, we use \textbf{quantum operations}
which we define to be \textbf{linear completely positive trace preserving} maps
or CPTP maps. These describe discrete changes in a system, and have the advantage
that they do no explicitly introduce time. Specifically, we consider these maps to
be maps of densities
\begin{equation}
  \Lambda: \mathcal{D}(\mathcal{H}) \to \mathcal{D} \mathcal{H}; \rho \mapsto \rho'
\end{equation}
satisfying the properties that $\Lambda$ is
\begin{itemize}
  \item linear (allowing us to interpret mixed states probabilistically)
  \item trace-preserving (we need to preserve total probability)
  \item positive (meaning that $\rho \geq 0 \implies \lambda(\rho) \geq 0$.
    Here $\rho \geq 0$ means $\rho$ is positive semi-definite.
  \item \textbf{complete positivity} which means that $\forall$ ancillas $B$
    the map $\Lambda \otimes \id_B$ is positive too. This is a stronger statement
    than positivity, since in particular we note that $\rho \mapsto \rho^T$ is not
    completely positive.
\end{itemize}

\begin{theorem}
  A map $\Lambda : \mathcal{B}(\mathcal{H}) \to \mathcal{B}(\mathcal{K})$ for 
  $\mathcal{H} \simeq \mathbb{C}^d$ is completely positive if and only if
  $$ (\Lambda \otimes \id_d) \ket{\Omega} \bra{\Omega} \geq 0 $$
  where $\ket{\Omega} = \frac{1}{\sqrt{d}} \sum \ket{ii} \in \mathbb{C}^d \times
  \mathbb{C}^d$ is the maximally entngled state of Schmidt rank $d$.
\end{theorem}
\begin{proof}
  Necessity of this condition is clear. To prove sufficiency we observe that 
  $\forall k \geq 1$ we have that $\rho \in \mathcal{D}(\mathcal{H} \otimes 
  \mathbb{C}^d)$ so we that if $\rho = \sum p_i \ket{\phi_i} \bra{\phi_i}$ then
  $$ \forall (\Lambda \otimes \id_k) \ket{\phi_i} \bra{\phi_i} \geq 0
  \implies (\Lambda \otimes \id_k) \rho \geq 0 $$
  From the last lecture we know that nay pure state can be written in the form
  $(I_d \otimes R) \ket{\Omega}$ for $R \in \mathcal{B}(\mathbb{C}^d, 
  \mathbb{C}^k)$ so in particular we see that $(\Lambda \otimes \id_k)
  \ket{\phi_i} \bra{\phi_i} \geq 0$ can be written in the forms
  \begin{align*}
    (\Lambda \otimes \id_k) (I_d \otimes R_i) \ket{\Omega} \bra{\Omega}
    (I_d \otimes R_i^\dagger) &\geq 0 \\
    (I_{d'} \otimes R_i) (\Lambda \otimes \id_d) \ket{\Omega} \bra{\Omega}
    (I_{d'} \otimes R_i^\dagger) &\geq 0
  \end{align*}
  meaning that since the outside conjugation is invertible, we are positive iff
  $$ (\Lambda \otimes \id_d) \ket{\Omega} \bra{\Omega} \geq 0 $$
  as required
\end{proof}
Note that here we call $J(\Lambda) = J = (\Lambda \otimes \id_d) \ket{\Omega}
\bra{\Omega}$ the \textbf{Choi matrix} or the \textbf{Choi state} of $\Lambda$.
[End of lecture 8]

We mentioned earlier that any quantum operation can be represented as a unitary
operation on a larger space. Formally, this is described by

\begin{theorem}
  \textbf{Stinespring's Dilation Theory} which states that for any CPTP operator
  $\Lambda: \mathcal{B(H)} \to \mathcal{B(H)}$ there exists 
  a Hilbert space $\mathcal{H'}$ and a unitary operator $U \in \mathcal{B(H 
  \otimes H')}$ such that for any $\rho \in \mathcal{D(H)}$
  \begin{equation}
    \Lambda(\rho) = \tr_{\mathcal{H}'}(U(\rho \otimes \varphi) U^\dagger)
  \end{equation}
  where $\varphi$ is fixed in $\mathcal{D(H')}$ (and can be chosen to be pure).
\end{theorem}

In essence this states that any quantum operation can be described by

\begin{enumerate}
  \item Adding an ancilla $\mathcal{H'}$
  \item Converting it to a unitary operator on the extended space ($U$)
  \item Removing the ancilla $\tr_{\mathcal{H'}}$
\end{enumerate}

\subsection{Krauss representation or operator sum representation}

Just a side note, $T : \rho \mapsto \rho^T$ is not CP since $T \otimes \id$
acts as a swap operator which has $-1$ as an eigenvalue.

\begin{theorem}
  The \textbf{Kraus Representation Theorem} states that $\Lambda : 
  \mathcal{B(H}_A) \to \mathcal{B(H}_B)$ is CPTP iff 
  \begin{equation}
    \Lambda(\rho) = \sum_k A_k \rho A_k^\dagger
  \end{equation}
  for some linear \textbf{Krauss operators} $A_k \in \mathcal{B(H}_A, 
  \mathcal{H}_B)$ satisfying $\sum A_k^\dagger A_k = I$.
\end{theorem}

In fact, this can be seen as a restatement/special case of Stinespring's
Dilation theorem since wlog we can choose the initial ancilla to be in the
state $\phi = \ket{\varphi} \bra{\varphi}$ so that if $\ket{e_k}$ is an 
orthonormal basis of the ancilla then
\begin{equation}
  A(\rho) = \sum_k \bra{e_k} U (\rho \otimes \varphi) U^\dagger \ket{e_k}
  = \sum_k A_k \rho A_k^\dagger
\end{equation}
where $A_k = \bra{e_k} U \ket{\phi}$ is an operator on the system state
(the ancilla part has already been evaluated). Then we have $\sum_k 
A_k^\dagger A_k = I$ by the completeness of $\ket{e_k}$ and $U$ is unitary
since $\ket\varphi$ is normalised. Finally, we see that $\Lambda(\rho)$ is 
certainly linear, and it is completely positive since
\begin{equation}
  (\Lambda \otimes \id_d) \ket\Omega \bra\Omega = \sum_k \bra\psi (A_k
  \otimes I) \ket\Omega \bra\Omega (A_k^\dagger \otimes I) \ket\psi =
  \sum_k \bra{\varphi_k} \ket\Omega \bra\Omega \ket{\varphi_k} \geq 0
\end{equation}
for $\ket{\varphi_k} = (A_k^\dagger \otimes I) \ket\psi$ since $\Omega =
\ket\Omega \bra\Omega$ is positive (semi-definite).

Basically, the Kraus representation is a way of achieving what Stinespring's
Dilation does without using an ancilla. Note that the Kraus representation
is not unique. [End of lecture 9]

\subsection{Choi-Jamilkowski Isomorphism}

\begin{theorem}
  The \textbf{Choi-Jamilkowski Isomorphism} states that there is an isomorphism
  between $\Lambda: \mathcal{M}_d \to \mathcal{M}_{d'}$ and $J \in \mathcal{B}
  (\mathbb{C}^{d'} \otimes \mathbb{C}^d)$ given by
  \begin{align}
    J &= (\Lambda \otimes \id_d) \ket\Omega \bra\Omega \\
    \tr(A \Lambda(B)) &= d \tr(J(A \otimes B^T))
  \end{align}
  where the last holds for all $A \in \mathcal{M}_{d'}, B \in \mathcal{M}_d$ and
  we have the properties
  \begin{enumerate}
    \item $\Lambda$ is completely positive iff $J \geq 0$
    \item $\Lambda$ is trace-preserving iff $\tr_A J = I / d$ where $\tr_A$ is 
      the partial trace on $\mathbb{C}^{d'}$
  \end{enumerate}
\end{theorem}

To justify this, we first define the \textbf{adjoint} of $\Lambda : \mathcal{M}_d
\to \mathcal{M}_{d'}$ to be $\mathcal{\Lambda}^*$ satisfying $\tr(A \Lambda(B)) =
\tr(\Lambda^*(A)B)$. Also, we note that complete positivity and trace-preserving 
follow from our earlier work. To establish that these are truly mutual inverses, 
let's start by reviewing the following identities (proof left as an exercise) 
for $A, B \in \mathcal{B}(\mathbb{C}^d)$:
\begin{itemize}
  \item $(A \otimes I) \ket\Omega = (I \otimes A^T) \ket\Omega$
  \item if $F\ket{ij} = \ket{ji}$, then $d\ket\Omega \bra\Omega^{T_B} = F$ where
    $T_B$ is the partial transpose defined such that for $C \in \mathcal{B(H}_A
    \otimes \mathcal{H}_B)$ has $\bra{ij} C^{T_A} \ket{kl} = \bra{kj} C \ket{il}$
  \item $\tr((A \otimes B)F) = \tr(AB)$
  \item $(A \otimes I)F = F(I \otimes A)$
\end{itemize}
As such, we argue that $\tr(A \Lambda(B)) = d\tr(J(A \otimes B^T))$ since 
\begin{align*}
  d\tr(J(A \otimes B^T))
  &= d \tr ((\Lambda \otimes \id_B) \ket\Omega \bra\Omega(A \otimes B^T)) \\
  &= d\tr((\Lambda \otimes \id_B) \frac{1}{d} F^{T_B} (A \otimes B^T)) \\
  &= \tr(F^{T_B} (\Lambda^* \otimes \id_B)(A \otimes B^T)) \\
  &= \tr(d \ket\Omega \bra\Omega (\Lambda^*(A) \otimes B^T)) \\
  &= d\tr(B\Lambda^*(A)) \\
  &= d\tr(A \Lambda(B))
\end{align*}
This shows we are an inverse in one direction, giving us injectivity in a certain
direction. As such it suffices to show that the map $\Lambda \to J$ is surjective,
but we can justify this by breaking $J$ into rank one $\ket\psi \bra\psi$ bits and
using $\ket\psi = (R \otimes I) \ket\Omega$ from before.

\subsection{From Choi-Jamilkowski to Kraus Representation to Stinespring's 
Dilation}

We note that it is possible to use the Choi-Jamilkowski Isomorphism to prove
the Kraus representation to prove Stinespring's Dilation theorem. We begin by
proving the Kraus representation from Choi-Jamilkowski
\begin{proof}
\item Note that $J(\Lambda) = \sum p_i \ket{\psi_i} \bra{\psi_i} \geq 0$ for
  $\ket{\psi_i} \in \mathcal{B(H}_A, \mathcal{H}_B)$, but we also know that we
  can write $\ket{\psi_i} = (R_i \otimes I) \ket\Omega$ for $R_i \in 
  \mathcal{B(H}_A, \mathcal H_B)$ so $J(\Lambda) = \sum_i (A_i \otimes I) \ket
  \Omega \bra\Omega (A_i^\dagger \otimes I)$ where $A_i = \sqrt{p_i} R_i$, and
  since $J \leftrightarrow \Lambda$ is an isomorphism so we can describe 
  $\Lambda$ entirely by its action on $\rho \in \mathcal{D(H}_A)$ as $\Lambda
  (\rho) = \sum A_i \rho A_i^\dagger$ as required. Furthermore, $\Lambda$ is
  trace preserving since $\forall \rho \in \mathcal{B(H}_A), \tr \Lambda(\rho)
  = \tr((\sum_i A_i^\dagger A_i) \rho) = \tr(\rho)$ meaning that since this holds
  for any $\rho$, we have $\sum A_i^\dagger A_i = I$. 
\end{proof}

For the proof of Stinespring's Dilation theorem from the Kraus representation
we see that
\begin{proof}
  For a Hilbert space $\mathcal H_E$ with dimension $r \geq \dim(J(\Lambda))$ 
  (why?) we can define $U$ by $\ket{\psi_A} \otimes \ket \varphi$ forall 
  $\ket{\psi_A} \in \mathcal H_A$ such that if $\varphi = \ket \varphi \bra
  \varphi$ then $U \bra{\psi_A} \otimes \bra \varphi = \sum_i A_i \bra{\psi_A}
  \otimes \ket i$ for an orthonormal basis $\ket i$ and Kraus operators $A_i$. 
  Then $U$ is an isomerty, and we see that for $\rho = \sum_i p_i \ket{\psi_i}
  \bra{\psi_i}$ then we have that $\tr_{\mathcal H_E}(U(\rho \otimes \phi) 
  U^\dagger) = \sum_i A_i \rho A_i^\dagger$ which is the same as $\Lambda(
  \rho)$ by Kraus representation.
\end{proof}

\end{document}
