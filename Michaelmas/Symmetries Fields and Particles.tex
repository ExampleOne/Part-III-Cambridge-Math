\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\usepackage{gensymb}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{adjustbox}
\usepackage{physics}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\title{Symmetries, Fields, and Particles}
\author{quinten tupker}
\date{October 10 2020 - \today}

\begin{document}

\maketitle

\section*{Introduction}

These notes are based on the course lectured by Ben Allanach in Michaelmas 2020.
Due to the measures taken in the UK to limit the spread of
Covid-19, these lectures were delivered online. These are not meant to be an
accurate representation of what was lectures. They solely represent a mix of
what I thought was the most important part of the course, mixed in with many
(many) personal remarks, comments and digressions... Of course, any
corrections/comments are appreciated.

To begin the course, the lecturer reminds of the definition of a group. I will
not repeat this definition here. Groups, and Lie groups in particular, are
essential in particle physics as a means of keeping track of the symmetries of
particles. Here we get two kinds of symmetries:

\begin{itemize}
\item An \textbf{internal symmetry} is an inherent property of the
  fields/particles themselves. For example, one can rotate through quark
  colours, and in fact, we find that in order to make this possible, we require
  the existence of a force carrying particle (a gluon) to be involved. And to
  conserve colours, gluons contain a mix of colours to do so. The Lie group
  structure enforces colour conservation here... Since these colour rotations can be different at
  different points in space and time, these symmetries can also be
  called a \textbf{local symmetry} or a \textbf{gauge symmetry}.
\item A \textbf{global symmetry} is a symmetry that leaves something the same
  across all space and time. 
\item A \textbf{external symmetry} is a symmetry involving spacetime
  coordinates. This includes symmetry under translation and Lorentz
  transformations. From these symmetries we get conserved structures (momentum,
  angular momentum, and energy). The Poincare group contains all these
  symmetries. 
\end{itemize}

In terms of particles, bosons carry forces, and these includes gluons (strong
force), photons (electromagnetic force), $Z^0, W^\pm$ carries the electroweak
force. It has also been hypothesised that the graviton (spin 2) carries gravity,
although it has never been observed. Also, for good symmetries, force carriers
should be massless, but the spontaneous symmetry breaking, through the Higgs
mechanism, can give force carriers mass (such as $W^\pm, Z^0$).

The lecturer provides a standard list of (elementary) fermions. I won't repeat
these.

Just as a note, in the standard model, every particle has a field, and
excitations of this field corresponds to ``instances'' of these particles.
[End of lecture 1]

In lecture 2, the lecturer reviews basic group theory. I won't repeat that here.
[End of lecture 2]

We continue looking at group properties, etc. Here are some definitions.

\begin{definition}
  The \textbf{inner automorphism} associated with $g \in G$ is $\phi_g(h) =
  ghg^{-1}$.
\end{definition}

We remark that treating elements of $G$ as automorphisms in this way we see that
$G / Z(G)$ is always a normal subgroup of $\text{Aut}(G)$.

\begin{definition}
  The \textbf{semi-direct product} of groups $H, G$, written $H \ltimes G$, has the
  product rule $(h, g)(h', g') = (hh', g\phi_h(g'))$, and inverse rule $(h,
  g)^{-1} = (h^{-1}, \phi_{h^{-1}}(g))$.
\end{definition}

here we can see that $H \equiv H \ltimes G / G$, and that $D_n \equiv
\mathbb{Z}_2 \ltimes \mathbb{Z}_n$ as expected.

\begin{definition}
  The \textbf{commutator subgroup} or \textbf{derived subgroup} of group $G$, denoted $[G, G]$,
  the group generated by the commutators of $G$. These are always normal, and a
  group is called \textbf{perfect} when it equals its own commutator subgroup.
\end{definition}

\section{Matrix Groups}

The lecturer describes the general linear group, orthogonal group, special
orthogonal group, unitary group, and special unitary group. He also describes
their dimensionality. In particular, using their properties, he shows that
$O(n)$ has $\frac{1}{2}n(n - 1)$ free parameters, and $SU(n)$ has $n^2 - 1$
parameters. He also mentions the famous fact that topologically $SU(2) \equiv
S^3$. [End of lecture 3]

Let's continue defining some special matrix groups that may be less well known.

\begin{definition}
  The \textbf{symplectic group} $S_p(2n, \mathbb{R})$ is a subset of the general
  linear group satisfying
  $$ M^T J M = J $$
  for
  $$ J =
  \begin{pmatrix}
    0 & -1 & 0 & 0 & \dots & 0 & 0 \\
    1 & 0 & 0 & 0 & \dots & 0 & 0 \\
    0 & 0 & 0 & -1 & \dots & 0 & 0 \\
    0 & 0 & 1 & 0 & \dots & 0 & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & 0 & \dots & 0 & -1 \\
    0 & 0 & 0 & 0 & \dots & 1 & 0
  \end{pmatrix}
  $$
\end{definition}

This group has dimension $n(2n - 1)$ when treated as a manifold, and the
determinant of every element in the group is 1 (applying the determinant to the
equation gives us $\pm1$, if you apply a generalised determinant for
anti-symmetric matrices called the Pfaffian you can see the sign as well). For
the sake of representations the most important thing about the symplectic
symplectic group is that unlike other groups we've seen, like $SO(n)$ or
$SU(n)$, the symplectic linear group is not compact.

\begin{definition}
  The pseudo-orthogonal group $O(n, m)$ is the subgroup of the general linear
  group satisfying
  $$ M^T g M = g $$
  for
  $$ g =
  \begin{pmatrix}
    I_n & \\
    & -I_m
  \end{pmatrix}
  $$
\end{definition}

Similarly we have $SO(n, m), U(n, m), SU(n, m)$. Also notice that $SO(1, 1) =
S_p(2, \mathbb{R})$.

\subsection{Representations}

A representation of a group is a homomorphism $\phi$ from that group to a space of
linear maps on some vector space $V$ on some field. For matrix groups, such as
those mentioned above, we 
also have the canonical \textbf{fundamental representation} that represents
matrices just as the matrices they are defined to be.

A representation is called \textbf{reducible} if we can find a linear subspace
so that the representation can be viewed purely as a representation on that
subspace (formally $\forall g \in G, u \in U, \phi(g) u \in U$ where $U \leq
V$). There are theorems in representation theory about the reducibility of
representations into irrudicible ones, such that we can always write

$$ V \equiv \bigoplus_r U_r $$

for irreducible $U_r$. A handy lemma in this field.

\begin{lemma}
  For irreducible representations $V, W$ of G,
  \begin{itemize}
  \item $Hom_G(V, W)$ ccontains only 0 or isomorphisms.
  \item if the field we are working on is algebraically closed, then
    $\text{dim} (Hom_G(V, W)) = 1$ 
  \end{itemize}
\end{lemma}

I know this has been worded in too abstract a manner, but it is more familiar
for me. Basically, two irreducible representations are either isomorphic,
meaning that they are the same up to similarity transformations (change of
bases), and there is only one similarity transformation up to multiplication by
a constant factor.

Despite the fact that any group has an infinite number of representations, a lot
of information can be summarised about them by calculating the
\textbf{character} $\chi(g) = tr(\phi(g))$, which is just the trace of every
matrix. Notice that since traces are unchanged by similarity transformations,
they firstly are the same for isomorphic/equivalent representations, and
secondly, are the same for conjugacy classes within the group, so they truly are
just summaries of the representation. Nonetheless, many important properties can
be deduced using them. For one, it is much easier to check if representations
are equivalent this way.

Finally, we consider the \textbf{tensor product} such that on $V \otimes W$
the product of two representations acts as

$$ (\phi \otimes \psi) (g) (v \otimes w) = \sum_{rs} \lambda_r \mu_s \phi(g)(v_r)
\otimes \psi(g)(w_s) = \phi(g)(v) \otimes \psi(g)(w) $$

where $v_r, w_s$ are the basis vectors of $V, W$ and $\lambda_r, \mu_s$ express
$v$, $w$ in these bases. [End of lecture 4]

A quick note is that one can calculate the tensor product on characters as

$$ \tr_{V_r \otimes V_s}(D^{R_r} (g) D^{R_s}(g)) = \tr_{V_r} (D^{R_r}(g))
\tr_{V_s} (D^{R_s}(g)) $$

Also, if $R_r \otimes R_s$ contains the singlet representation then we can
construct an inner product $\langle v, v' \rangle$ which is invariant under the
group.

\subsection{Symmetries and Quantum Mechanics}

Alright, now let's actually start using some of these mathematical ideas!

A (unitary) map $U$ is called a \textbf{symmetry} if $|\langle \phi | \psi
\rangle = |\langle U\phi, U\psi \rangle|^2$ for all $\phi, \psi$. Furthermore,
one can show that $U$ is always linear or anti-linear (\textbf{anti-linear}
means it is a linear map except it applies complex conjugation, so $Ua\ket{\psi}
= a^*U\ket{\psi}$). Furthermore, it can be shown that anti-linear maps are only
linear when we consider time reversal. Therefore, we will focus on linear maps.

One can also show that such maps, if they represent a symmetry group $G$ satisfy
the product rule

$$U(g) U(g') = e^{i\gamma(g, g')}U(gg')$$

where we usually just assume that $\gamma = 0$ leaving us with a homomorphism.
One can further show that it must always commute with the Hamiltonian if it is a
symmetry so

$UH = HU$

meaning that we can find a simultaneous eigenbasis for the two.

\section{Rotations $SO(3)$ and $SU(2)$}

An important group of symmetries are the group of rotations $SO(3)$ and $SU(2)$.
We will prove an important results on them, namely that

$$ SO(3) \equiv SU(2) / \mathbb{Z}_2 $$

To do so we will develop an important tool, namely Pauli matrices. But before we
start, let's review some standard properties of these groups. In $SO(3)$ we can
write a general rotation by $\theta$ about axis $n$ as

$$ R_{ij} = \cos(\theta) \delta{ij} + (1 - \cos(\theta)) n_i n_j - \sin(\theta)
\epsilon_{ijk} n_k $$

which corresponds infinitesimally to

$$ \delta x = \delta n \times x. $$

Now, let's start proving this isomorphism. First, the Pauli matrices are:

$$
\sigma_1 =
\begin{pmatrix}
  & 1 \\
  1 & \\
\end{pmatrix},
\sigma_2 =
\begin{pmatrix}
  & -i \\
  i & \\
\end{pmatrix},
\sigma_3 =
\begin{pmatrix}
  1 & \\
  & -1 \\
\end{pmatrix}
$$

which satisfy the properties

$$ \sigma_i \sigma_j = \delta_{ij}I + \epsilon_{ijk} \sigma_k $$

and

$$ \tr(\sigma_i \sigma_j) = 2 \delta_{ij} $$

In other words, what exactly are the Pauli matrices? The Pauli matrices form an
orthogonal basis (up to a factor of 2) when we treat the space of Hermitian traceless
$2\times 2$ matrices as a real vector space. Hermitian means the diagonal
entries must be real, and traceless means the top left and top right entries
must add to 0. This is generated by $\sigma_3$. Meanwhile, $\sigma_1, \sigma_2$
handle the off-diagonal basis. Why are they orthogonal? The natural inner
product to use on matrix spaces is to rearrange the matrices into long vectors,
and just to apply the dot-product to these vectors, and that can exactly be
written as

$$ A \cdot B = \tr{AB} = A_{ij} B_{ij} $$

Under this product, the property $ \tr(\sigma_i \sigma_j) = 2 \delta{ij}$ means
these matrices truly are orthogonal. Now, we can easily form a complete basis
for the space of $2 \times 2$ Hermitian matrices by adding the identity $I$.
When doing so, we find that any matrix $A \in SU(2)$ can be written in this
basis as

$$ A = \frac{1}{2} \tr(A) I + \frac{1}{2} \tr(\sigma_i A) \sigma_i $$

Note in particular, that $\tr(A) = \tr(AI)$, so that if we define $\sigma_0 = I$
we get

$$ A = \frac{1}{2} \tr{\sigma_\mu A} \sigma_\mu, \,\, \mu = 0, 1, 2, 3 $$

which is really just writing it in a new orthogonal basis according to a certain
dot product. [End of lecture 5]

So what is the isomorphism we are going to use. Consider the map

$$ x \mapsto x_i \sigma_i $$

which has inverse

$$ x_i \sigma_i \mapsto \frac{1}{2} \tr(\sigma_j x_i \sigma_i) $$

and satisfies $(x_i \sigma_i)^2 = x^2 I$, and $x_i \sigma_i$ has eigenvalues
$\pm\sqrt{x^2}$, leaving determinant $\det(x_i \sigma_i) = -x^2$.

Now, for any $A \in SU(2)$ let $x \mapsto x'$ via

$$ x'_i \sigma_i = Ax_i \sigma_i A^\dagger $$

which has $x'^2 = -det{x'_i \sigma_i} = \det{x_i \sigma_i} = x^2$ since $A$ is
unitary. That means this map preserves length and so sits in $O(3)$. How do we
show it is a rotation, not a reflection? We notice that the determinant as a map
is continuous, and as $A \to I$, $R_A \to I$ as well, so we must get $\det{R_A}
= 1$. So if we assume that $SU(2)$ has only a single connected component, we see
that we map entirely onto $SO(3)$.

Now, we'd like to invert this map as well, and a first step for us would be to
see if we can write this map down explicitly. To do so, we notice that

$$ (Rx)_i \sigma_i = \sigma_i R_{ij} x_j = x'_i \sigma_i = Ax_j \sigma_j
A^\dagger $$

which is true for any $x$, meaning that

$$ \sigma_i R_{ij} = A \sigma_j A^\dagger $$

Here, the LHS is just a decomposition of a matrix into a sum of Pauli, matrices,
so taking a dot product (trace) with the appropriate matrices to undo the
composition we get

$$ R_{ij} = \frac{1}{2} \tr(\sigma_i A \sigma_j A^\dagger) $$

which gives us one direction for our map. To reverse this, we notice that
$\sigma_i \sigma_j \sigma_i = -\sigma_j$, and $\sigma_i^2 = I$, meaning that
$\sigma_\mu \sigma_\nu \sigma_\mu = 4\delta_{0 \nu} I$. 

$$ \sigma_\mu A^\dagger \sigma_\mu = 2 \tr(A) I \implies \sigma_i A^\dagger
\sigma_i = 2 \tr(A) - \sigma_0 A^\dagger \sigma_0 $$

(the implication refers to an equation in lectures).

Applying this to our formula for $R_{ij}$ we gets

$$ R_{jj} = \frac{1}{2} \tr(\sigma_j A \sigma_j A^\dagger) = \frac{1}{2}
\tr(\sigma_\mu A \sigma_\mu A^\dagger - \sigma_0 A \sigma_0 A^\dagger) =
\tr(\tr(A) A^\dagger) - \frac{1}{2} \tr(AA^\dagger) = |\tr(A)|^2 - 1 $$

Furthermore, we get

\begin{align*}
\sigma_i R_{ij} \sigma_j
&= \frac{1}{2} \sigma_i \tr(\sigma_i A \sigma_j A^\dagger) \sigma_j \\
&= \frac{1}{8} \sigma_i \tr(\sigma_i \tr(\sigma_\mu A) \sigma_\mu \sigma_j \tr(\sigma_\nu A^\dagger) \sigma_\nu) \sigma_j \\
&= \dots \\
&= 2\tr(A)A - I
\end{align*}

(I am not sure on the proof of this). These together leave

$$ A = \pm \frac{I + \sigma_i R_{ij} \sigma_j}{2\sqrt{1 + R_{jj}}} $$

which completes our isomorphism.

We also find we can express these relations efficiently infinitesimally:

$$ R_{ij} = \delta_{ij} - \delta \theta \epsilon_{ijk} n_k $$

$$ A = I - i \delta \theta n_k \sigma_k / 2 $$

since $\det(I + X) = 1 + \tr(X)$ meaning that in order to keep $\det = 1$ to
first order we need $\tr(X) = 0$ above. Now, we can recover an arbitrary
rotation by exponentiation, and then see that

$$ A(\theta, n) = e^{-\frac{i}{2} n_k \sigma_k \theta} = 2 \cos(\theta / 2) - i
  n_k \sigma_k \sin(\theta / 2) $$

This makes it clearer where the $\mathbb{Z}_2$ symmetry comes from.

\subsection{Infinitesimal Generators and Rotations}

Now for rotations $R_1 = R(\delta \theta_1, n_1), R_2 = R(\delta \theta_2, n_2)$
we find that commutator $R_c = R_2^{-1}R_1^{-1}R_2R_1$ is given infinitesimally
by

$$ x \mapsto x + \delta \theta_1 \theta_2 (n_2 \times (n_1 \times x) - n_1
\times (n_2 \times x)) = x + \delta \theta_1 \delta \theta_2 $$

In general, in quantum mechanics we assume any unitary operation has a
generator, such as for rotations

$$ U(R(\delta \theta, n)) = I - i \delta \theta n \cdot J $$

for generators $J$ of rotations about each axis. Note that in general, $U$
unitary means that generators $J_i$ are all Hermitian. As such we see that

$$ U(R_c) = I - i \delta \theta_1 \delta \theta_2 (n_2 \times n_1) \cdot J = 1 -
\delta \theta_1 \delta \theta_2 [n_2 \cdot J, n_1 \cdot J_2] $$

which gives us the famous commutation relation

$$ [J_i, J_j] = i \epsilon_{ijk} J_k $$

Similarly, for a function of space, where we use $L$ instead of $J$ as a
generator of rotations we gets

$$ f \mapsto (1 - i \delta \theta \cdot n L) f(x) $$

which gives rise to thse same commutation relations as we had before. We can
then recover finite rotations by exponentiating as expected. Now, we also see
that rotations correspond to conservation of angular momentum since they
conserve the Hamiltonian.

\subsection{Representations of the Angular Momentum Commutation Relations}

For convenience when dealing with the angular momentum operator (generator of
rotations) we we define $J_{\pm} = J_1 \pm iJ_2$ (which become our ladder
operators), and gives rise to the commutation relations

$$ [J_3, J_\pm] = \pm J_\pm $$

$$ [J_+, J_-] = 2 J_3 $$

and of course, $J_3^* = J_3, J_+^* = J_-$. We then pick our basis, which leads
us to

$$ J_3 \ket{m} = m \ket{m} $$

$$ J_+ \ket{m} = \lambda_m \ket{m \pm 1} $$

$$ J_- \ket{m} = \ket{m - 1} $$

or 0 (sometimes $J_\pm$ annihilates a state). Using commutation relations we can
then deduce that

$$ \lambda_m = j(j + 1) - m(m + 1). $$

[End of lecture 6]

\end{document}