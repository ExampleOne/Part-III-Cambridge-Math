\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\usepackage{gensymb}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{adjustbox}
\usepackage{physics}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\title{Symmetries, Fields, and Particles}
\author{quinten tupker}
\date{October 10 2020 - \today}

\begin{document}

\maketitle

\section*{Introduction}

These notes are based on the course lectured by Ben Allanach in Michaelmas 2020.
Due to the measures taken in the UK to limit the spread of
Covid-19, these lectures were delivered online. These are not meant to be an
accurate representation of what was lectures. They solely represent a mix of
what I thought was the most important part of the course, mixed in with many
(many) personal remarks, comments and digressions... Of course, any
corrections/comments are appreciated.

To begin the course, the lecturer reminds of the definition of a group. I will
not repeat this definition here. Groups, and Lie groups in particular, are
essential in particle physics as a means of keeping track of the symmetries of
particles. Here we get two kinds of symmetries:

\begin{itemize}
\item An \textbf{internal symmetry} is an inherent property of the
  fields/particles themselves. For example, one can rotate through quark
  colours, and in fact, we find that in order to make this possible, we require
  the existence of a force carrying particle (a gluon) to be involved. And to
  conserve colours, gluons contain a mix of colours to do so. The Lie group
  structure enforces colour conservation here... Since these colour rotations can be different at
  different points in space and time, these symmetries can also be
  called a \textbf{local symmetry} or a \textbf{gauge symmetry}.
\item A \textbf{global symmetry} is a symmetry that leaves something the same
  across all space and time. 
\item A \textbf{external symmetry} is a symmetry involving spacetime
  coordinates. This includes symmetry under translation and Lorentz
  transformations. From these symmetries we get conserved structures (momentum,
  angular momentum, and energy). The Poincare group contains all these
  symmetries. 
\end{itemize}

In terms of particles, bosons carry forces, and these includes gluons (strong
force), photons (electromagnetic force), $Z^0, W^\pm$ carries the electroweak
force. It has also been hypothesised that the graviton (spin 2) carries gravity,
although it has never been observed. Also, for good symmetries, force carriers
should be massless, but the spontaneous symmetry breaking, through the Higgs
mechanism, can give force carriers mass (such as $W^\pm, Z^0$).

The lecturer provides a standard list of (elementary) fermions. I won't repeat
these.

Just as a note, in the standard model, every particle has a field, and
excitations of this field corresponds to ``instances'' of these particles.
[End of lecture 1]

In lecture 2, the lecturer reviews basic group theory. I won't repeat that here.
[End of lecture 2]

We continue looking at group properties, etc. Here are some definitions.

\begin{definition}
  The \textbf{inner automorphism} associated with $g \in G$ is $\phi_g(h) =
  ghg^{-1}$.
\end{definition}

We remark that treating elements of $G$ as automorphisms in this way we see that
$G / Z(G)$ is always a normal subgroup of $\text{Aut}(G)$.

\begin{definition}
  The \textbf{semi-direct product} of groups $H, G$, written $H \ltimes G$, has the
  product rule $(h, g)(h', g') = (hh', g\phi_h(g'))$, and inverse rule $(h,
  g)^{-1} = (h^{-1}, \phi_{h^{-1}}(g))$.
\end{definition}

here we can see that $H \equiv H \ltimes G / G$, and that $D_n \equiv
\mathbb{Z}_2 \ltimes \mathbb{Z}_n$ as expected.

\begin{definition}
  The \textbf{commutator subgroup} or \textbf{derived subgroup} of group $G$, denoted $[G, G]$,
  the group generated by the commutators of $G$. These are always normal, and a
  group is called \textbf{perfect} when it equals its own commutator subgroup.
\end{definition}

\section{Matrix Groups}

The lecturer describes the general linear group, orthogonal group, special
orthogonal group, unitary group, and special unitary group. He also describes
their dimensionality. In particular, using their properties, he shows that
$O(n)$ has $\frac{1}{2}n(n - 1)$ free parameters, and $SU(n)$ has $n^2 - 1$
parameters. He also mentions the famous fact that topologically $SU(2) \equiv
S^3$. [End of lecture 3]

Let's continue defining some special matrix groups that may be less well known.

\begin{definition}
  The \textbf{symplectic group} $S_p(2n, \mathbb{R})$ is a subset of the general
  linear group satisfying
  $$ M^T J M = J $$
  for
  $$ J =
  \begin{pmatrix}
    0 & -1 & 0 & 0 & \dots & 0 & 0 \\
    1 & 0 & 0 & 0 & \dots & 0 & 0 \\
    0 & 0 & 0 & -1 & \dots & 0 & 0 \\
    0 & 0 & 1 & 0 & \dots & 0 & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & 0 & \dots & 0 & -1 \\
    0 & 0 & 0 & 0 & \dots & 1 & 0
  \end{pmatrix}
  $$
\end{definition}

This group has dimension $n(2n - 1)$ when treated as a manifold, and the
determinant of every element in the group is 1 (applying the determinant to the
equation gives us $\pm1$, if you apply a generalised determinant for
anti-symmetric matrices called the Pfaffian you can see the sign as well). For
the sake of representations the most important thing about the symplectic
symplectic group is that unlike other groups we've seen, like $SO(n)$ or
$SU(n)$, the symplectic linear group is not compact.

\begin{definition}
  The pseudo-orthogonal group $O(n, m)$ is the subgroup of the general linear
  group satisfying
  $$ M^T g M = g $$
  for
  $$ g =
  \begin{pmatrix}
    I_n & \\
    & -I_m
  \end{pmatrix}
  $$
\end{definition}

Similarly we have $SO(n, m), U(n, m), SU(n, m)$. Also notice that $SO(1, 1) =
S_p(2, \mathbb{R})$.

\subsection{Representations}

A representation of a group is a homomorphism $\phi$ from that group to a space of
linear maps on some vector space $V$ on some field. For matrix groups, such as
those mentioned above, we 
also have the canonical \textbf{fundamental representation} that represents
matrices just as the matrices they are defined to be.

A representation is called \textbf{reducible} if we can find a linear subspace
so that the representation can be viewed purely as a representation on that
subspace (formally $\forall g \in G, u \in U, \phi(g) u \in U$ where $U \leq
V$). There are theorems in representation theory about the reducibility of
representations into irrudicible ones, such that we can always write

$$ V \equiv \bigoplus_r U_r $$

for irreducible $U_r$. A handy lemma in this field.

\begin{lemma}
  For irreducible representations $V, W$ of G,
  \begin{itemize}
  \item $Hom_G(V, W)$ ccontains only 0 or isomorphisms.
  \item if the field we are working on is algebraically closed, then
    $\text{dim} (Hom_G(V, W)) = 1$ 
  \end{itemize}
\end{lemma}

I know this has been worded in too abstract a manner, but it is more familiar
for me. Basically, two irreducible representations are either isomorphic,
meaning that they are the same up to similarity transformations (change of
bases), and there is only one similarity transformation up to multiplication by
a constant factor.

Despite the fact that any group has an infinite number of representations, a lot
of information can be summarised about them by calculating the
\textbf{character} $\chi(g) = tr(\phi(g))$, which is just the trace of every
matrix. Notice that since traces are unchanged by similarity transformations,
they firstly are the same for isomorphic/equivalent representations, and
secondly, are the same for conjugacy classes within the group, so they truly are
just summaries of the representation. Nonetheless, many important properties can
be deduced using them. For one, it is much easier to check if representations
are equivalent this way.

Finally, we consider the \textbf{tensor product} such that on $V \otimes W$
the product of two representations acts as

$$ (\phi \otimes \psi) (g) (v \otimes w) = \sum_{rs} \lambda_r \mu_s \phi(g)(v_r)
\otimes \psi(g)(w_s) = \phi(g)(v) \otimes \psi(g)(w) $$

where $v_r, w_s$ are the basis vectors of $V, W$ and $\lambda_r, \mu_s$ express
$v$, $w$ in these bases. [End of lecture 4]

A quick note is that one can calculate the tensor product on characters as

$$ \tr_{V_r \otimes V_s}(D^{R_r} (g) D^{R_s}(g)) = \tr_{V_r} (D^{R_r}(g))
\tr_{V_s} (D^{R_s}(g)) $$

Also, if $R_r \otimes R_s$ contains the singlet representation then we can
construct an inner product $\langle v, v' \rangle$ which is invariant under the
group.

\subsection{Symmetries and Quantum Mechanics}

Alright, now let's actually start using some of these mathematical ideas!

A (unitary) map $U$ is called a \textbf{symmetry} if $|\langle \phi | \psi
\rangle = |\langle U\phi, U\psi \rangle|^2$ for all $\phi, \psi$. Furthermore,
one can show that $U$ is always linear or anti-linear (\textbf{anti-linear}
means it is a linear map except it applies complex conjugation, so $Ua\ket{\psi}
= a^*U\ket{\psi}$). Furthermore, it can be shown that anti-linear maps are only
linear when we consider time reversal. Therefore, we will focus on linear maps.

One can also show that such maps, if they represent a symmetry group $G$ satisfy
the product rule

$$U(g) U(g') = e^{i\gamma(g, g')}U(gg')$$

where we usually just assume that $\gamma = 0$ leaving us with a homomorphism.
One can further show that it must always commute with the Hamiltonian if it is a
symmetry so

$UH = HU$

meaning that we can find a simultaneous eigenbasis for the two.

\section{Rotations $SO(3)$ and $SU(2)$}

An important group of symmetries are the group of rotations $SO(3)$ and $SU(2)$.
We will prove an important results on them, namely that

$$ SO(3) \equiv SU(2) / \mathbb{Z}_2 $$

To do so we will develop an important tool, namely Pauli matrices. But before we
start, let's review some standard properties of these groups. In $SO(3)$ we can
write a general rotation by $\theta$ about axis $n$ as

$$ R_{ij} = \cos(\theta) \delta{ij} + (1 - \cos(\theta)) n_i n_j - \sin(\theta)
\epsilon_{ijk} n_k $$

which corresponds infinitesimally to

$$ \delta x = \delta n \times x. $$

Now, let's start proving this isomorphism. First, the Pauli matrices are:

$$
\sigma_1 =
\begin{pmatrix}
  & 1 \\
  1 & \\
\end{pmatrix},
\sigma_2 =
\begin{pmatrix}
  & -i \\
  i & \\
\end{pmatrix},
\sigma_3 =
\begin{pmatrix}
  1 & \\
  & -1 \\
\end{pmatrix}
$$

which satisfy the properties

$$ \sigma_i \sigma_j = \delta_{ij}I + \epsilon_{ijk} \sigma_k $$

and

$$ \tr(\sigma_i \sigma_j) = 2 \delta_{ij} $$

In other words, what exactly are the Pauli matrices? The Pauli matrices form an
orthogonal basis (up to a factor of 2) when we treat the space of Hermitian traceless
$2\times 2$ matrices as a real vector space. Hermitian means the diagonal
entries must be real, and traceless means the top left and top right entries
must add to 0. This is generated by $\sigma_3$. Meanwhile, $\sigma_1, \sigma_2$
handle the off-diagonal basis. Why are they orthogonal? The natural inner
product to use on matrix spaces is to rearrange the matrices into long vectors,
and just to apply the dot-product to these vectors, and that can exactly be
written as

$$ A \cdot B = \tr{AB} = A_{ij} B_{ij} $$

Under this product, the property $ \tr(\sigma_i \sigma_j) = 2 \delta{ij}$ means
these matrices truly are orthogonal. Now, we can easily form a complete basis
for the space of $2 \times 2$ Hermitian matrices by adding the identity $I$.
When doing so, we find that any matrix $A \in SU(2)$ can be written in this
basis as

$$ A = \frac{1}{2} \tr(A) I + \frac{1}{2} \tr(\sigma_i A) \sigma_i $$

Note in particular, that $\tr(A) = \tr(AI)$, so that if we define $\sigma_0 = I$
we get

$$ A = \frac{1}{2} \tr{\sigma_\mu A} \sigma_\mu, \,\, \mu = 0, 1, 2, 3 $$

which is really just writing it in a new orthogonal basis according to a certain
dot product. [End of lecture 5]

So what is the isomorphism we are going to use. Consider the map

$$ x \mapsto x_i \sigma_i $$

which has inverse

$$ x_i \sigma_i \mapsto \frac{1}{2} \tr(\sigma_j x_i \sigma_i) $$

and satisfies $(x_i \sigma_i)^2 = x^2 I$, and $x_i \sigma_i$ has eigenvalues
$\pm\sqrt{x^2}$, leaving determinant $\det(x_i \sigma_i) = -x^2$.

Now, for any $A \in SU(2)$ let $x \mapsto x'$ via

$$ x'_i \sigma_i = Ax_i \sigma_i A^\dagger $$

which has $x'^2 = -det{x'_i \sigma_i} = \det{x_i \sigma_i} = x^2$ since $A$ is
unitary. That means this map preserves length and so sits in $O(3)$. How do we
show it is a rotation, not a reflection? We notice that the determinant as a map
is continuous, and as $A \to I$, $R_A \to I$ as well, so we must get $\det{R_A}
= 1$. So if we assume that $SU(2)$ has only a single connected component, we see
that we map entirely onto $SO(3)$.

Now, we'd like to invert this map as well, and a first step for us would be to
see if we can write this map down explicitly. To do so, we notice that

$$ (Rx)_i \sigma_i = \sigma_i R_{ij} x_j = x'_i \sigma_i = Ax_j \sigma_j
A^\dagger $$

which is true for any $x$, meaning that

$$ \sigma_i R_{ij} = A \sigma_j A^\dagger $$

Here, the LHS is just a decomposition of a matrix into a sum of Pauli, matrices,
so taking a dot product (trace) with the appropriate matrices to undo the
composition we get

$$ R_{ij} = \frac{1}{2} \tr(\sigma_i A \sigma_j A^\dagger) $$

which gives us one direction for our map. To reverse this, we notice that
$\sigma_i \sigma_j \sigma_i = -\sigma_j$, and $\sigma_i^2 = I$, meaning that
$\sigma_\mu \sigma_\nu \sigma_\mu = 4\delta_{0 \nu} I$. 

$$ \sigma_\mu A^\dagger \sigma_\mu = 2 \tr(A) I \implies \sigma_i A^\dagger
\sigma_i = 2 \tr(A) - \sigma_0 A^\dagger \sigma_0 $$

(the implication refers to an equation in lectures).

Applying this to our formula for $R_{ij}$ we gets

$$ R_{jj} = \frac{1}{2} \tr(\sigma_j A \sigma_j A^\dagger) = \frac{1}{2}
\tr(\sigma_\mu A \sigma_\mu A^\dagger - \sigma_0 A \sigma_0 A^\dagger) =
\tr(\tr(A) A^\dagger) - \frac{1}{2} \tr(AA^\dagger) = |\tr(A)|^2 - 1 $$

Furthermore, we get

\begin{align*}
\sigma_i R_{ij} \sigma_j
&= \frac{1}{2} \sigma_i \tr(\sigma_i A \sigma_j A^\dagger) \sigma_j \\
&= \frac{1}{8} \sigma_i \tr(\sigma_i \tr(\sigma_\mu A) \sigma_\mu \sigma_j \tr(\sigma_\nu A^\dagger) \sigma_\nu) \sigma_j \\
&= \dots \\
&= 2\tr(A)A - I
\end{align*}

(by taking out the traces inside, and analysing case-by-case one can find nice
expressions for the trace of products of Pauli matrices). These together leave

$$ A = \pm \frac{I + \sigma_i R_{ij} \sigma_j}{2\sqrt{1 + R_{jj}}} $$

which completes our isomorphism.

We also find we can express these relations efficiently infinitesimally:

$$ R_{ij} = \delta_{ij} - \delta \theta \epsilon_{ijk} n_k $$

$$ A = I - i \delta \theta n_k \sigma_k / 2 $$

since $\det(I + X) = 1 + \tr(X)$ meaning that in order to keep $\det = 1$ to
first order we need $\tr(X) = 0$ above. Now, we can recover an arbitrary
rotation by exponentiation, and then see that

$$ A(\theta, n) = e^{-\frac{i}{2} n_k \sigma_k \theta} = 2 \cos(\theta / 2) - i
  n_k \sigma_k \sin(\theta / 2) $$

This makes it clearer where the $\mathbb{Z}_2$ symmetry comes from.

\subsection{Infinitesimal Generators and Rotations}

Now for rotations $R_1 = R(\delta \theta_1, n_1), R_2 = R(\delta \theta_2, n_2)$
we find that commutator $R_c = R_2^{-1}R_1^{-1}R_2R_1$ is given infinitesimally
by

$$ x \mapsto x + \delta \theta_1 \theta_2 (n_2 \times (n_1 \times x) - n_1
\times (n_2 \times x)) = x + \delta \theta_1 \delta \theta_2 $$

In general, in quantum mechanics we assume any unitary operation has a
generator, such as for rotations

$$ U(R(\delta \theta, n)) = I - i \delta \theta n \cdot J $$

for generators $J$ of rotations about each axis. Note that in general, $U$
unitary means that generators $J_i$ are all Hermitian. As such we see that

$$ U(R_c) = I - i \delta \theta_1 \delta \theta_2 (n_2 \times n_1) \cdot J = 1 -
\delta \theta_1 \delta \theta_2 [n_2 \cdot J, n_1 \cdot J_2] $$

which gives us the famous commutation relation

$$ [J_i, J_j] = i \epsilon_{ijk} J_k $$

Similarly, for a function of space, where we use $L$ instead of $J$ as a
generator of rotations we gets

$$ f \mapsto (1 - i \delta \theta \cdot n L) f(x) $$

which gives rise to thse same commutation relations as we had before. We can
then recover finite rotations by exponentiating as expected. Now, we also see
that rotations correspond to conservation of angular momentum since they
conserve the Hamiltonian.

\subsection{Representations of the Angular Momentum Commutation Relations}

For convenience when dealing with the angular momentum operator (generator of
rotations) we we define $J_{\pm} = J_1 \pm iJ_2$ (which become our ladder
operators), and gives rise to the commutation relations

$$ [J_3, J_\pm] = \pm J_\pm $$

$$ [J_+, J_-] = 2 J_3 $$

and of course, $J_3^* = J_3, J_+^* = J_-$. We then pick our basis, which leads
us to

$$ J_3 \ket{m} = m \ket{m} $$

$$ J_+ \ket{m} = \lambda_m \ket{m \pm 1} $$

$$ J_- \ket{m} = \ket{m - 1} $$

or 0 (sometimes $J_\pm$ annihilates a state). Using commutation relations we can
then deduce that

$$ \lambda_m = j(j + 1) - m(m + 1). $$

(since we see that $\lambda_{m - 1} - \lambda_m = 2m$)

[End of lecture 6]

We notice that this means that for large $m$ $\lambda_m$ becomes negative,
however, $J_\pm J_\mp$ is non-negative definite meaning that all eigenvalues
must be non-negative. Consequently there must exist $m_\pm$ such that $J_\pm
\ket{m_\pm} = 0 = (j \mp m_\pm)(j \pm m_\pm + 1)$. This means that $m_\pm = \pm
j$ for $j \in \{0, 1/2, 1, 3/2, \dots\}$. These states form an orthogonal basis
for a representation $\mathcal{V}_j$ of dimension $2j + 1$.

\subsubsection{The $\ket{j m}$ Basis}

[Explain why we need these representations?]

We consequently get an orthogonal $\ket{jm}$ basis such that $J_3 \ket{j\,m} = m
\ket{j m}$. Here we can define a \textbf{highest weight state} to be $\ket{j\,j}$.
Now

$$ J_\pm \ket{j\,m} = \sqrt{(j - m)(j + m + 1)} \ket{j\, m \pm 1} $$

means that we can define states as

$$ (J_-)^n \ket{j \, j} = \sqrt{\frac{n! (2j)!}{(2j - m)!}} \ket{j \, j - n} $$

That is one way of defining $j$, but of course, we also have the more classical
picture using the Casimir operator

$$ J^2 = J_i J_i = J_+ J_- + J_3^2 - J_3 $$

If we do that we see that $J\ket{j\, j} = j(j + 1) \ket{j\, j}$. But we notice
that $[J^2, J_-] = 0$ meaning that in general

$$ J^2 \ket{j \, m} = j(j + 1) \ket{j\, m} $$

meaning that indeed, $J^2$ can be considered to correspond to the total angular
momentum. 

Now that we've sorted out the overall algebraic properties that our
representation should have, let's figure out what the actual representations of
this space are. Here we see that we get

\begin{align*}
J^{(j)}_{3 \, m'm} &= m \delta_{m'm} \\
J^{(j)}_{\pm \, m'm} = \sqrt{(j \mp m)(j \pm m + 1)} \delta_{m'm \pm 1}
\end{align*}

which completes our representation for rotation generators $J$ in the basis such
that our matrices are $\bra{j \, m'} J_i \ket{j \, m}$. We'd like to define a
similar representation for finite rotations $D(R)$ such that infinitesimally

$$ D^{(j)}(R(\delta n, n)) = I - i \delta \theta n \cdot J^{(j)} $$

To get the finite version, we consider the Euler angles for an arbitrary
rotation such that

$$ R = R(\phi, e_3) R(\theta, e_2) R(\psi, e_3) $$

giving representation

$$ U(R) = e^{-i \phi J_3} e^{-i\theta J_2} e^{-i\psi J_3} $$

meaning that

$$ D^{(j)}_{m'm} = e^{-i m' \phi - i m \psi} d^{(j)}_{m'm}(\theta) $$

for

$$ d^{(j)}_{m'm}(\theta) = \bra{j \, m'} e^{-i \theta J_2} \ket{j \, m} $$

A few special cases can be worked out as $d^{(j)}_{m'm}(\pi) = (-1)^{j - m}
\delta_{m', -m}$ and $d^{(j)}_{m'm} (2\pi) = (-1)^{2j} \delta_{m'm}$, and in
fact in general

$$ D^{(j)}(R(2\pi, n)) = (-1)^{2j} I $$

where we note crucially that $j \in \{0, 1/2, 1, 3/2, \dots\}$. We also find the
general result that

$$ d^{(j)}_{m'm} (\theta) = (-1)^{m' - m} d^{(j)}_{-m', -m}(\theta) = (-1)^{m' -
  m} d^{(j)}_{m'm} (-\theta) = (-1)^{m' - m} d^{(j)}_{mm'}(\theta) $$

implying a degree of symmetry at least.

Furthermore, we see that in the special case $j=1/2$ we get $J_i = \sigma_i$ the
Pauli matrices, and

$$ d^{(1/2)}(\theta) =
\begin{pmatrix}
  \cos(\theta / 2) & -\sin(\theta / 2) \\
  \sin(\theta / 2) & \cos(\theta / 2)
\end{pmatrix}
$$

and we can generate the character of a rotation by angle $\theta$ to in general
be (independent of the axis of rotation)

$$ \chi_j(\theta) = \frac{\sin((j + 1/2) \theta)}{\sin(\theta / 2)} $$

\subsubsection{Tensor Products and Angular Momentum Addition}

Now a somewhat well known decomposition is what happens when you add to
representations together and look at the new representation structure. In
particular, how do we look at the tensor product $\mathcal{V}_{j_1} \otimes
\mathcal{V}_{j_2}$ with basis $\ket{j_1 \, m_1} \ket{j_2 \, m_2}$. Then we can
extend our generators to the new space via 

$$ J_1 = J_1 \otimes I_2 $$
$$ J_2 = I_1 \otimes J_2 $$

[End of lecture 7]

We then form a new basis $\ket{J\, M}$ which we define to satisfy

$$ J_3 \ket{J \, M} = M \ket{J \, M} $$
$$ J^2 \ket{J \, M} = J(J + 1) \ket{J \, M} $$

but of course, we also have the natural basis inherited from the definition of
the space: $\ket{j_1 \, m_1 \, j_2 \, m_2}$, so how do these relate? We define
the \textbf{Clebsch-Gordan} coefficients to be precisely to coefficients
expressing $\ket{J\, M}$ in terms of $\ket{j_1 \, m_1 \, j_2 \, m_2}$, where we
note in particular that in order to make $J_3$ values match Clebsch-Gordan
coefficients are only nonzero when $m_1 + m_2$.

Since we also inherit the raising and lowering operators, we might hope that we
can, just as with a pure state $\mathcal{V}_j$, find a highest weight state
satisfying 

$$ J_3 \ket{J\, J} = J \ket{J \, J}$$
$$ J_+ \ket{J \, J} = 0$$

By thinking a little, we can see that the following definitely works

$$ \ket{j_1 + j_2 \, j_1 + j_2} = ket{j_1\, j_1} \ket{j_2\, j_2} $$

We can then use $J_-$ to find the rest of the coefficients. Note that we could
also have started with a state annihilated by $J_-$ instead and then use $J_+$
instead, although it really comes down to the same (but perhaps this implies
some symmetry in the Clebsch-Gordan coefficients?). 

Now, if we define $\mathcal{V}^{(M)}$ to be eigenspace of $J_3$ with eigenvalue
$M$ then, by counting on the original space, we see that

$$ \text{dim}(\mathcal{V}^{(M)}) = 
\begin{cases}
j_1 + j_2 - M + 1 & M \geq j_1 - j_2 \\
2j_2 + 1 & M \leq j_1 - j_2
\end{cases}
$$

where we've assumed without loss of generality that $j_1 \geq j_2$. Now, from
the above we know how we ``travel'' inside of $\mathcal{V}^{(M)}$ for fixed $M$,
but we have yet to see how we travel between different values of $J$. We do not
have a particularly efficient way of doing so: we only see that by exploiting
dimensions efficiently we see that $\ket{j_1 + j_2 \, j_1 + j_2 - 1}$ is
orthogonal to $\ket{j_1 + j_2 - 1 \, j_1 + j_2 - 1}$, and similarly, we
gradually can find Clebsch-Gordan coefficients for $J < j_1 + j_2$.

By further considering the original spaces, we see that $J \in \{j_1 + j_2,
\dots, \abs{j_1 - j_2}\}$. Using $\text{dim}(\mathcal{V}^{(J)}) = 2J + 1$ we
can check we do get the correct dimension of our space $(2j_1 + 1)(2j_2 + 2)$.
[The lecturer mentions some straightforward orthogonality relationships, and
gives an example for $j_1 = 1, j_2 = 1/2$.]

\subsection{$SO(3)$ tensors}

When studying how a certain system $S$ behaves there are two questions one might
wish to consider. Firstly, how does $S$ behave as the input is varied? This
generally is the topic of calculus, and differential equations, etc. Secondly,
under what transformations does $S$ stay the same? This is generally the topic
of group theory and the like. This course certainly focuses more on the second
question, and a concept that illustrates this perhaps the best is that of
tensors. Tensors, after all, essentially are things that are left invariant
under certain group transformations. The numerical representations of these
states change under these transformations, but tensors essentially define
themselves as objects whose ``true'' value is unchanged under these
transformations.

The simplest type of tensors for us to consider are tensors under rotations, so
tensors under $SO(3)$. Here, we can see that the matrix multiplying indices are
the representation of the group, while the tensors themselves inhabit the linear
space that that group is acting on. As such tensors are just representations of
the group that determines their transformation rule, and as such we might ask
what spaces of tensors are irreducible representations? If we denote the space
of rank-$n$ tensors as $\mathcal{V}^{\otimes n}$, then we see in particular that
isotropic tensors form irreducible representations. As such one can deduce that
all rank 2 tensors split into the following irreducible representations: For any
rank-2 tensors $T_{ij}$

$$ T_{ij} = S_{ij} + \epsilon_{ijk} v_k + \frac{1}{3} \delta_{ij} T_kk $$

where $S_{ij}$ is a symmetric, traceless tensor, $v_k$ is a rank-1 tensor
describing the anti-symmetric part of $T_{ij}$ and the last term captures the
trace.

To generalise we'd like to be able to find an easy test of irreducibility on
these spaces. Here we see that for candidate space $S_{i_1 \dots i_n}$ we see
that $\delta_{i_k i_{k'}} S_{i_1 \dots i_n} = 0$ always if $S_{i_1 \dots i_n}$
is irreducible (otherwise the space of such nonzero tensors would generate a
subspace). As such we one such irreducible space is totally symmetric and
traceless. 

Finally, we remark that summing over all indices forms an invariant inner
product on the space. [End of lecture 8] 

What is the dimension of a totally symmetric traceless tensor? In thrs, any
symmetric tensor can be written in the form

$$ S_{\underbrace{1 \dots 1}_{r \text{ times}} \underbrace{2 \dots 2}_{s \text{
      times}} \underbrace{3 \dots 3}_{t \text{ times}}} $$

by roerdering, meaning that

$$ \text{dim}(\text{Sym}(\mathcal{V}^n)) = \frac{(n + 1)(n + 2)}{2} $$

If we also impose that we have to traceless, we get dimension $2n + 1$ as we
would expect for a representation of angular momentum eigenstates. 

\subsection{Spinors}

It turns out that the spinor representation can just be expressed as the
fundamental representation of $SU(r)$ with tensors transforming via $\eta_\alpha
\mapsto \eta'_\alpha = A_\alpha^\beta \eta_\beta$ for $A \in SU(r)$. $A \in
SU(r) \implies (A_\alpha^\beta)^* = (A^{-1})_\alpha^\beta$ (notice the
transpose) which corresponds to the conjugate representation $\bar{\eta}^\alpha
= (\eta_\alpha)^*$.

For $SU(2)$, which we will focus on, $\epsilon^{\alpha \beta} = -\epsilon^{\beta
\alpha}$ is the only isotropic tensor. By convention we choose $\epsilon^{12} =
1, \epsilon_{12} = -1$. To check it is truly isotropic notice that

$$ \epsilon'_{\alpha \beta} = A_\alpha^\gamma A_\beta^\delta \epsilon_{\gamma
  \delta} = \text{det}(A) \epsilon_{\alpha \beta} = \epsilon_{\alpha \beta} $$

Now, again, we wish to find the irreducible representations for this group. In
this case, we see that anything contracted with $\epsilon^{\alpha \beta}$ must
be zero (or else it could be used to form a proper-subrepresentation). As such,
we find that any irreducible $S_{\alpha_1 \dots \alpha_n}$ must be totally
symmetric, as before. Using a similar counting method as before, we find that
the dimension of such spaces is $n + 1$ for $j = n/2$ as one might hope. 

Now, so far we have to consider both $\eta_\alpha$ and $\bar{eta}^\alpha$ but we
notice that we can use $\epsilon^{\alpha \beta}$ to raise and lower indices. As
such, it is sufficient to only consider $\eta_\alpha$. Also, notice that that

$$ \epsilon_{\alpha \beta} \epsilon^{\gamma \delta} = -\delta_\alpha^\gamma
\delta_\beta^\delta + \delta_\alpha^\delta \delta_\beta^\gamma $$

Now, writing 

$$ \delta_{\eta_\alpha} = - i \delta \theta \frac{1}{2} (n \cdot
\theta)_\alpha^\beta \eta_\beta $$

so that for an arbitrary tensors
$$ \delta T_{\alpha_1 \dots \alpha_m} = -i \delta \theta \sum_r \frac{1}{2} (n
\cdot \sigma)_\alpha^\beta (T_{\alpha_1 \dots \alpha_{r - 1} \beta \alpha_{r +
    1} \dots \alpha_m}) $$

meaning that $\epsilon^{\alpha \gamma} \epsilon_{\beta \delta}
\sigma_\gamma^\delta = \sigma_\beta^\alpha$ so that $\tr(\sigma) = 0$ and
$\epsilon^{\alpha \beta} \sigma_\gamma^\beta = \epsilon^{\beta \gamma}
\sigma_\gamma^\alpha$. We then can write the completeness of Pauli matrices
as

$$ (\sigma \epsilon)_{\alpha \beta} \cdot (\sigma \epsilon)^{\gamma \delta} =
\delta_\alpha^\gamma \delta_\beta^\delta + \delta_\alpha^\delta
\delta_\beta^\gamma $$

$$ (\epsilon \sigma)^{\alpha \beta} \cdot (\epsilon \sigma)^{\gamma \delta} =
-\epsilon^{\alpha \gamma} \epsilon^{\beta \delta} - \epsilon^{\alpha \gamma}
\epsilon^{\beta \gamma} $$

The Pauli matrices are crucial since they link $SU(2)$ back to $SO(3)$ which we
are relying on. In particular we see that for $n$ even wecan write

$$ T_{i_1 \dots i_n} = (\epsilon \sigma_{i_1})^{\alpha_1 \beta_1} \dots
(\epsilon \sigma_{i_n})^{\alpha_n \beta_n} S_{\alpha_1 \dots \alpha_n \beta_1
  \dots \beta_n} $$

[do we mean $n/2$ in the last term here?] and for $n$ odd we see that

$$ T_{i_1 \dots i_n} = \epsilon^{\beta_1 \gamma_1} \dots \epsilon^{\beta_r
  \gamma_r} S_{1, (\alpha_1 \dots \alpha_{n - r} \beta_1 \dots \beta_r} S_{2,
  \alpha_{n - r + 1} \dots \alpha_{n + m - 2r})\gamma_1 \dots \gamma_r} $$

for $S_{1, \alpha_1 \dots \alpha_r}$, $S_{2, \beta_1 \dots \beta_m}$ and $n \geq
m, r$. In particular for $n=m=1, r=0, 1$ for $u_\alpha, v_\alpha$ we find that

$$ u_\alpha v_\beta = u_{(\alpha} v_{\beta)} + \frac{1}{2} \epsilon_{\alpha
  \beta} u_\beta^\gamma v_\gamma $$

where we see that $u_{(\alpha} v_{\beta)}$ is a vector in $SO(3)$. This means
that

$$ t_i = (\epsilon \sigma_i)^{\alpha \beta} u_{(\alpha} v_{\beta)} $$

is a tensor and we can reverse this via

$$ u_\alpha v_\beta = \frac{1}{2} t_i (\epsilon \sigma_i)_{\alpha \beta} +
\frac{1}{2} \epsilon_{\alpha \beta} u^\gamma v_\gamma $$

where we in effect see $j = 1 / 2 \otimes j = 1/2 \equiv j = 1 \oplus j = 0$.

\section{Relativistic Symmetries}

All that we've dealt so far are classical symmetries with the standard rotation
group. But really, we'd like to redo this work relativistically. As such, and
sticking to special relativity, we upgrade to the Lorentz group (and sometimes
Poincar\'{e} group). 

\subsection{The Lorentz Group}

We wish to define a group that preserves an inner product given by $x^\mu, x^\nu
\mapsto g_{\mu \nu} x^\mu x^\nu$ where $g_{00} = 1, g_{0i} = g_{i0} = 0, g_{ii}
= -1$. By linear algebra we know that any such inner product is equivalent to
the inner product with 

$$ g = 
\begin{pmatrix}
1 & & & \\
& -1 & & \\
& & -1 & \\
& & & -1
\end{pmatrix} $$

Once we deice to preserve this metric, we will now show that this indeed turns
out to be the Lorentz group. Firstly, we argue that isometries must be linear
(or affine really - which corresponds to Poincar\'{e}). If $x'^2 = x^2$ and
$x'^\mu = x^\mu + f^\mu(x)$ for some $f$ under some transformation then we see
that we get  $dx'^\mu = dx^\mu + \partial_\sigma f^\mu(x) dx^\sigma$ meaning
that $\partial_\mu f_\nu + \partial_\nu f_\mu = 0$ which is \textbf{Killing
  equation}. Working a little more we see that

$$ 2\partial_\omega \partial_\mu f_\nu = \partial_\omega(\partial_\nu\\mu f_\nu
+ \partial_\nu f_\mu) + \partial_\mu (\partial_\nu f_\omega + \partial_\omega
f_\nu) - \partial_\nu(\partial_\omega f_\mu + \partial_\mu f_\omega) = 0 $$

meaning that all second derivatives of $f$ are 0 so f is indeed affine

$$ f_\mu = a_\mu + \omega_{\mu \nu} x^\nu $$

where the killing identity forces $\omega_{\mu \nu}$ to be asymmetric. If we
then focus on the linear part, we see that we do indeed get Lorentz
transformations. However, we notice that topologically, the space of Lorentz
transformations splits into 4 connected components given by $\Lambda^0_{\,0}
\geq 1$ or $\Lambda^0_{\,0} \leq -1$ and $\det(\Lambda) = \pm 1$. Notice also
that $(\Lambda^0_{\,0})^2 = 1 + \sum_{i = 1}^3 (\Lambda^0_i)^2$ (why?). Anyways,
once we require preservation of parity and casaulity we find ourselves
restricted to $\Lambda^0_0 \geq 1, \det(\Lambda) = 1$. We call this restricted
group $SO(1, 3)$ the \textbf{proper orthochronous Lorentz group}. Rotations of
this group forma  subgroup as 

$$ \Lambda_R =
\begin{pmatrix}
1 & \\
& R 
\end{pmatrix} $$

We can also write all Lorentz boosts in the form

$$ \Lambda_B = 
\begin{pmatrix}
\cosh(\theta) & \sinh(\theta) n \\
\sinh(\theta) n & B 
\end{pmatrix} $$

for symmetric $B$ and unit vector $n$. Writing the preservation of metric $g$ we
find that $Bn = \cosh(\theta)n, B^2 = -\sinh^2(\theta) n n^T = I$ meaning that

$$ B = I + (\cosh(\theta) - 1) n n^T $$

a Lorentz boost with velocity $v = \tanh(\theta)$ in direction $n$. These don't
form a subgroup (although all Lorentz boosts restricted to the same direction
$n$ do form a subgroup). Now, importantly we notice that we can write any
Lorentz transformation as a rotation followed by Lorentz boost. We do this by
noticing that all rotations are all orthogonal in the Lorentz group and all
Lorentz boosts are symmetric. As a result for any $\Lambda \in SO(1, 3)$ we can
define $\Lambda_B = \sqrt{\Lambda^T \Lambda}$ and the we see that

$$ (\Lambda(\Lambda_B)^{-1})^T (\Lambda (\Lambda_B)^{-1}) = I $$

Now that we've established the basic structure of the Lorentz group, and its
origins, let's see what kind of quantum theory we can build off of this. In
particular, let's start with the commutation relations where we see that for

\begin{align*}
\Lambda^\mu_{1\nu} &= \delta^\mu_\nu + \omega^\mu_{1\nu} \\
\Lambda^\mu_{2\nu} &= \delta^\mu_\nu + \omega^\mu_{2\nu} 
\end{align*}

Then we get that

$$ \Lambda^\mu_\nu = (\lambda_2^{-1} \Lambda_1^{-1} \Lambda_2 \Lambda_1)^\mu_\nu
= \delta^\mu_\nu + [\omega_2, \omega_1]^\mu_\nu $$

For a representation $U$ of our theory we get that

$$ U(\Lambda) = 1 - \frac{i}{2} \omega^{\mu \nu} M_{\mu \nu} $$

for antisymmetri $M_{\mu \nu}$. We then see that

$$ U(\Lambda) = 1 - i[\omega_2, \omega_1]^{\mu \nu} M_{\mu \nu} = 1 -
\frac{1}{2} [\omega^\mu_{2 \rho \sigma}, \omega^\mu_{1\sigma \rho}] $$

so in particular

$$ [\omega_2, \omega_1]^{\mu \nu} = g_{\sigma \rho}(\omega_2^{\mu \sigma}
\omega_1^{\sigma \nu} - \omega_1^{\mu \sigma} \omega_2^{\rho \nu}) $$

Consequently we find

$$ [M_{\mu \nu}, M_{\sigma \rho}] = i [g_{\nu \sigma} M_{\mu \rho} - g_{\mu
  \sigma} M_{\nu \rho} - g_{\nu \rho} M_{\mu \sigma} + g_{\nu \rho} M_{\nu
  \sigma}] $$

We now can also define tensors to be \textbf{contravariant} if they transform
as

$$ U(\Lambda) V^\mu U(\Lambda)^{-1} = (\Lambda^{-1})^\mu_\nu V^\nu $$

and \textbf{covariant} if they transform according to

$$ U(\Lambda) U_\mu U(\Lambda)^{-1} = U_\nu \Lambda^\nu_\mu $$

Infinitesimally we get

$$ [M_{\mu \nu}, V^\sigma] = -i (\delta^\sigma_\mu V_\nu - \delta^\sigma_\nu
V_\mu) $$

$$ [M_{\mu \nu}, U_\sigma] = -i (g_{\mu \sigma} U_\nu - g_{\nu \sigma} U_\mu) $$

Restricting to $ijkl$ as indices (spatial only) we see that if we define $J_m =
\frac{1}{2} \epsilon_{mij} M_{ij}$ then $M_{ij} = \epsilon_{ijm} J_m$ leaving

$$ [J_m, J_n] = i \epsilon_{mnl} J_l $$

(representing angular moment) leaving

$$ [M_{0i}, M_{0j}] = -i (\delta_{jk} M_{0i} - \delta_{ik} M_{0j}) $$

and

$$ [M_{0i}, M_{0j}] = -i M_{ij} $$

meaning that if $K_i = M_{0i}$ so $K_i^\dagger = K_i$ (?)

$$ [J_i, K_j] = i \epsilon_{ijk} K_k, [K_i, K_j] = -i \epsilon_{ijk} J_k $$

meaning $K_i$ is a well-defined vector operation under the $SO(1, 3)$ structure
(what does this mean? - I need to look at Hugh Osborne's notes). As expected,
infinitesimally we get $\delta x^\mu = \omega^\mu_\nu x^\nu, \omega_{ij} =
\epsilon_{ijk} \theta_k$ and so if $\omega^0_i - \omega^i_0 = v_i$ then $\delta
t = v \cdot x, \delta x = v t + \theta \times x$. As such we can write for a
general Lorentz transformation that

$$ U(\Lambda) = 1 - i \theta \cdot J + i v \cdot k $$

where $J$ is the rotation and $k$ is the boost. Now writing

$$ J^\pm = \frac{1}{2} (J \pm iK) $$

then we get

$$ [J_i^\pm, J_j^\pm] = i \epsilon_{ijk} J_k^\pm $$

an $SU(2) \times SU(2)$ structure. 

Now, just as we could get spinors for $SO(3)$ we can also get spinors for $SO(1,
3)$. Now instead of using just the Pauli-matrices, we need to use the
Pauli-matrices plus identity, as used earlier. In particular, we write

$$ \sigma_\mu = (I, \sigma_1, \sigma_2, \sigma_3) = \sigma_\mu^\dagger $$

$$ \bar{\sigma}_\mu = (I, -\sigma_1, -\sigma_2, -\sigma_3) =
\bar{\sigma}_\mu^\dagger $$

We still get identities like

$$ \sigma_i \sigma_j = \delta_{ij} I + i \epsilon_{ijk} \sigma_k $$

$$ \sigma_\mu \bar{\sigma}_\nu + \sigma_\nu \bar{\sigma}_\mu = 2 g_{\mu \nu} I =
\bar{\sigma}_\mu \sigma_\nu + \bar{\sigma}_\nu \sigma_\mu $$

$$ \tr(\sigma_\mu \bar{\sigma}_\nu) = 2g_{\mu \nu} $$

$$ A = \frac{1}{2} \tr(\bar{\sigma}^\mu A) \sigma_\mu $$

Now, just as before we have an isomorphism between $SO(1, 3) \equiv SL(2,
\mathbb{C}) / \mathbb{Z}_2$ by extending the previous isomorphism. In
particular, we see that we can map 4-vectors to $2 \times 2$-matrices by

$$ x^\mu \mapsto \sigma_\mu x^\mu = x $$

which is reverse by

$$ x^\mu = \frac{1}{2} \tr(\bar{\sigma}^\mu x) $$

Explicitly one can find that

$$ x = 
\begin{pmatrix}
x^0 + x^3 & x^1 - ix^2 \\
x^1 + ix^2 & x^0 - x^3 
\end{pmatrix} $$

Now, as before we also have $\det(x) = x^2 = g_{\mu \nu} x^\mu x^\nu$ and
furthermore, for $\bar{x} = \bar{\sigma}_\mu x^\mu$ we have $x\bar{x} = \bar{x}x
= x^2 I$. Now, for any $A \in SL(2, \mathbb{C})$ we can define $x \mapsto x' =
AxA^\dagger = x'^\dagger$. $\det(A) = 1 \implies x^2 = x'^2$ and so there exists
a Lorentz transform such that

$$ x'^\mu = \Lambda^\mu_\nu x^\nu, \sigma_\mu\Lambda^\mu_\nu = A \sigma_\nu
A^\dagger $$

so

$$ \Lambda^\mu_\nu = \frac{1}{2} \tr(\bar{\sigma^\mu A \sigma_\nu A^\dagger}) $$

[End of lecture 10]. To reverse this map, we notice that

$$ \sigma_\nu A^\dagger \bar{\sigma}^\nu = 2\tr(A^\dagger) I \implies A^\mu_\mu
= |\tr(A)|^2 \implies \sigma_\mu A^\sigma_{\ v}\bar{\sigma}^\nu $$

As such, we find that

$$ A = \frac{e^{i\alpha} \sigma_\mu \Lambda^\mu_{\ \nu} \bar{\sigma}^\nu}{2
  \sqrt{\Lambda^\mu_\mu}} $$

where $\tr(A) = e^{i\alpha} |\tr(A)|$ and $\alpha$ can be determined from
$\det(A) = 1$ up to a factor of $\pm1$ (giving us the necessary homomorphism and
kernel). 

That completes our correspondance. Now for some final remarks, note that for
$A^\dagger = A^{-1}$ (so $A \in SU(2)$ (and not just $SL(2)$)), $x'^0 = x^0$ so
we get a rotation of $x$. Similarly, we see that if $A^\dagger = A$ then
$\Lambda$ is symmetric, meaning we get a Lorentz boost. In particular, we
observe that

$$ A_B = \cosh(\theta / 2) I + \sinh(\theta / 2) n \cdot \sigma $$ 

corresponds to $\Lambda_B$. 

\subsection{2-Spinors and dotted indices}

As usual for spinors we require

\begin{align*}
\psi_\alpha &\mapsto \Lambda_\alpha^{\ \beta} \psi_\beta \\
\chi^\alpha &\mapsto \chi^\beta (\Lambda^{-1})_\beta^{\ \alpha}
\end{align*}

for $A \in SL(2, \mathbb{C})$. Note that these are used in the super-symmetry
and standard model courses. As usual we use the totally antisymmetric
$\epsilon^{\alpha \beta}$ tensor to raise/lower indeices. However, there is one
difference with what we had before. Now, we notice that

$$ \epsilon^{\alpha \gamma} A_\gamma^{\ \delta} \epsilon_{\delta \beta} =
\delta_\beta^\alpha \tr(\Lambda) - \Lambda_\beta^{\ \alpha} $$

and since $\det(A) = 1$ we notice that writing the characteristic equation we
have 

$$ A^2 - A \tr(A) + \det(A) I = 0 \implies A(\tr(A) I - A) = I \implies A^{-1} =
\tr(A) I - A $$

meaning that

$$ \epsilon^{\alpha \gamma} A_\gamma^{\ \delta} \epsilon_{\delta \beta} =
(A^{-1})_\beta^{\ \alpha} $$

unlike in $SO(3)$. Because of this we get two inequivalent representations under
conjugation, and so get two fundamentally different types of spinors. To
distinguish these, we add dots to our indices so that now, when we are in the
conjugate space we write

$$ \bar{\psi}_{\dot{\alpha}} = (\psi_\alpha)^*, \bar{\chi}^{\dot{\alpha}} =
(\chi^\alpha)^* $$

which transform via

\begin{align*}
\bar{\psi}_{\dot{\alpha}} &\mapsto \bar{\psi}_{\dot{\beta}} (\bar{A}^{-1})^{\dot{\beta}}_{\ \dot{\alpha}} \\
\bar{\chi}^{\dot{\alpha}} &\mapsto \bar{A}^{\dot{\alpha}}_{\ \dot{\beta}} \bar{\chi}^{\dot{\beta}} 
\end{align*}

where 

$$ (\bar{A}^{-1})^{\dot{\alpha}}_{\ \dot{\beta}} = (A_\beta^{\ \alpha})^* $$

or $\bar{A}^{-1} = A^\dagger$ [?]. Here we note that for $A_1, A_2 \in SL(2,
\mathbb{C})$ we have $\overline{A_1 A_2} = \bar{A}_1 \bar{A}_2$ and that just as
before we can use $\epsilon^{\dot{\alpha}\dot{\beta}}$ and
$\epsilon_{\dot{\alpha \dot{\beta}}}$ to raise and lower indices. 

One weird effect we do get is that now the Pauli matrices have mixed type
indices in the sense that

\begin{align*}
\sigma_\mu &= (\sigma_\mu)_{\alpha \dot{\alpha}} \\
\bar{\sigma}_\mu &= (\bar{\sigma}_\mu)^{\dot{\alpha} \alpha} = \epsilon^{\dot{\alpha} \dot{\beta}} \epsilon^{\alpha \beta} (\sigma_\mu)_{\beta \dot{\beta}} 
\end{align*}

Note that here $\alpha, \dot{\alpha}$ are different indices, so we do not sum
over them, even if they are placed next to each other. We consequently find that

$$ A \sigma_\nu \bar{A}^{-1} = \sigma_\mu \lambda^\mu_{\ \nu} \implies \bar{A}
\bar{\sigma}_\nu A^{-1} = \bar{\sigma}_\mu \Lambda^\mu_{\ \nu} $$

Furtheremore, we can combine these spinors into 4-component \textbf{Dirac
  spinors} as 

$$ \Phi = 
\begin{pmatrix}
\psi_\alpha \\
\bar{\chi}^{\dot{\alpha}}
\end{pmatrix} $$

and 

$$ \bar{\Phi} = 
\begin{pmatrix} \chi^\alpha & \bar{\psi}_\alpha \end{pmatrix}
= \bar{\Phi}^\dagger 
\begin{pmatrix}
& 1 \\
1 & 
\end{pmatrix} $$

The $4 \times 4$ Dirac matrices then are (generalise the Pauli matrices)

$$ \gamma_\mu = 
\begin{pmatrix}
& \sigma_\mu \\
\bar{\sigma}_\mu & 
\end{pmatrix} $$

and $\{\gamma_\mu, \gamma_\nu\} = 2g_{\mu \nu} I$ (making something called a
\textbf{Clifford algebra}?). The tensorial representations associated with these
are

\begin{align*}
T^{\mu_1 \dots \mu_\nu} & \mapsto \Lambda^{\mu_1}_{\ \nu_1} \dots \Lambda^{\mu_n}_{\ \nu_n} T^{\nu_1 \dots \nu_n} \\
\gamma_{\alpha_1 \dots \alpha_{2j}, \dot{\alpha}_1 \dots \dot{\alpha}_{2 \bar{j}}} & \mapsto A_{\alpha_1}^{\ \beta_1} \dots A_{\alpha_{2j}}^{\ \beta_{2j}} \gamma_{\beta_1 \dots \beta_{2j}, \dots{\beta}_1 \dots \dots{\beta}_{2j}} (\bar{A}^{-1})^{\dots{\beta}_1}_{\ \dots{\alpha}_1} \dots (\bar{A}^{-1})^{\dot{\beta}_{2\bar{j}}}_{\ \dot{\alpha}_{2 \bar{j}}}
\end{align*}

To find the irreducible tensors, since we're now in 4 dimensions we need to
consider the following isotropic tensors:

$$ g^{\mu \nu}, \epsilon_{\alpha \beta}, \epsilon_{\dot{\alpha} \dot{\beta}},
\epsilon^{\mu \nu \rho \sigma} $$

where the last symbol is totally asymmetric in all four indices. As such the
irreducible representations are just the irreducible tensors that are totally
symmetric in both types of indices (allowing all swaps within the sets of
undotted and undotted indices, but not between them) leaving us with

$$ \gamma_{\alpha_1 \dots \alpha_{2j}, \dot{\alpha}_1 \dots \dot{\alpha}_{2
    \bar{j}}} = \gamma_{(\alpha_1 \dots \alpha_{2j}, (\dot{\alpha}_1 \dots
\dot{\alpha}_{2\bar{j}}))} $$

Consequently we can label all irreducible spinorial representations of $SO(1,
3)$ by $(j, \bar{j})$ (which corresponds to $(\bar{j}, j)$ under complex
conjugation). The dimension of such a representation is $(2j + 1)(2\bar{j} + 1)$
as one might expect. Here, the ``fundamental'' spinors are (0, 1/2) and (1/2, 0)
although the Dirac spinor is $(0, 1/2) \oplus (1/2, 0)$. This is used more in
the Standard Model course.

Just as with $SO(3)$ we can decompose a tensor product of irreducible
representations into a sum of irreducible representations

$$ (j_1, \bar{j}_1) \otimes (j_2, \bar{j}_2) \equiv \bigoplus_{|j_1 - j_2| \leq
  j \leq |j_1 + j_2|, |\bar{j}_1 - \bar{j}_2| \leq \bar{j} \leq |\bar{j}_1 +
  \bar{j}_2 |} (j, \bar{j}) $$

For $2j = n = 2\bar{j}$ we can relate this simply back to rank $n$ vectorial
tensors via

$$ T_{\mu_1 \dots \mu_n} = \gamma_{\alpha_1 \dots \alpha_n, \dot{\alpha}_1 \dots
\dot{\alpha}_n} (\bar{\sigma}_{\mu_1})^{\dot{\alpha}_1 \alpha_1} \dots
(\bar{\sigma}_{\mu_n})^{\dot{\alpha}_n \alpha_n} $$

If $\gamma$ is irreducible the $T$ here is symmetric and traceless.

\subsection{The Poincar\'{e} group}

The Poincar\'{e} group which is the Lorentz group with translations added is
often written as $ISO(1, 3)$ with the extra requirement that parity is
preserved, so $\det(\Lambda) = 1$. Elements of this group are denoted by
$(\Lambda, a)$ where the action

$$ (\Lambda, a) x^\mu = \Lambda^\mu_{\ \nu} x^\mu + a^\nu $$

The composition rule used can be written in this notation as

$$ (\Lambda_1, a_1) (\Lambda_2, a_2) = (\Lambda_2 \Lambda_1, \Lambda_2 a_1 +
a_2) $$

We also notice that the identity element is just $(I, 0)$ and inverses are
$(\Lambda, a)^{-1} = (\Lambda^{-1}, -\Lambda^{-1} a)$. Obviously, a general
element in the Poincar\'{e} group can be written as the product of a Lorentz
transform and a translation as $(\Lambda, a) = (I, a) (\Lambda, 0)$.
Furthermore, commutators are of the form that if 

$$ (\Lambda, a) = (\Lambda_2, a_2)^{-1} (\Lambda_1, a_1)^{-1} (\Lambda_2, a_2)
(\Lambda_1, a_1) $$

then

\begin{align*}
\Lambda &= \Lambda_2^{-1} \Lambda_1^{-1} \Lambda_2 \Lambda_1 \\
a &= \Lambda_2^{-1} \Lambda_1^{-1} (\Lambda_2 a_1 - \Lambda_1 a_2 - a_1 + a_2) 
\end{align*}

Infinitesimally, we find that

$$ \Lambda^\mu_{i\ \nu} = \delta^\mu_\nu + \omega^\mu_{i\ \nu} \implies
\Lambda^\mu_{\ \nu} = \delta^\mu_\nu + [\omega_2, \omega_1]^\mu_{\ \nu}, a^\mu
= \omega^\mu_{2\ \nu} a^\nu_1 - \omega^\mu_{1\ \nu} a^\nu_2 $$

In our quantum theory we then get

$$ U(\Lambda, a) = 1 - \frac{i}{2} \omega^{\mu \nu} M_{\mu \nu} + i a^\mu
P_\mu $$

for generators of translation $P_\mu$. We then consider

\begin{align*}
  U(\Lambda, a) &= 1 - i[\omega_2, \omega_1]^{\mu \nu} M_{\mu \nu} + i (\omega_2 a_1 - \omega_1 a_2)^\mu P_\mu \\
                &= U(\Lambda_2, a_2)^{-1} UU(\Lambda_1, a_1)^{-1} U(\Lambda_2, a_2) U(\Lambda_1, a_1) \\
                &= \dots \\
                &= 1 - [\frac{1}{2} \omega_2^{\mu \nu} M_{\mu \nu} - a_2^\mu P_\mu, \frac{1}{2} \omega_1^{\omega \rho} M_{\omega \rho} - a_1^\sigma P_\sigma]
\end{align*}

From here we can deduce commutation relations

$$ [\frac{1}{2} \omega_1^{\sigma \rho} M_{\sigma \rho}, a_2^\mu P_\mu] = i
(\omega_1 a_2)^{\mu} P_\mu $$

$$ [a_2^\mu P_\mu a_1^\nu P_\nu] = 0 $$

and since the $\omega_i, a_i$ are arbitrary we find

$$ [M_{\mu \nu}, P_\sigma] = i (g_{\nu \sigma} P_\mu - g_{\mu \sigma} P_\nu),
[P_\mu, P_\nu] = 0 $$

which confirms that we have proper 4-vector operators [how?]. Consequently
writing

$$ (\Lambda, 0) (I, a) (\Lambda, 0)^{-1} = (I, \Lambda a) \implies U(\Lambda, 0)
P_\mu U(\Lambda, 0)^{-1} = P_\nu \Lambda^\nu_{\ \mu} $$

and then decomposing $P^\mu = (H, \vec{P}), P_\mu = (H, -\vec{P})$ leaves us
with

$$ [J_i, H] = 0, [J_i, P_j] = i \epsilon_{ijk} P_k, [K_i, H] = iP_i, [K_i, P_j]
= i \delta_{ij} H $$

as before [End of lecture 11]. 
                  

\end{document}