\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\usepackage{gensymb}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{adjustbox}
\usepackage{physics}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\title{Symmetries, Fields, and Particles}
\author{quinten tupker}
\date{October 10 2020 - \today}

\begin{document}

\maketitle

\section*{Introduction}

These notes are based on the course lectured by Ben Allanach in Michaelmas 2020.
Due to the measures taken in the UK to limit the spread of
Covid-19, these lectures were delivered online. These are not meant to be an
accurate representation of what was lectures. They solely represent a mix of
what I thought was the most important part of the course, mixed in with many
(many) personal remarks, comments and digressions... Of course, any
corrections/comments are appreciated.

To begin the course, the lecturer reminds of the definition of a group. I will
not repeat this definition here. Groups, and Lie groups in particular, are
essential in particle physics as a means of keeping track of the symmetries of
particles. Here we get two kinds of symmetries:

\begin{itemize}
\item An \textbf{internal symmetry} is an inherent property of the
  fields/particles themselves. For example, one can rotate through quark
  colours, and in fact, we find that in order to make this possible, we require
  the existence of a force carrying particle (a gluon) to be involved. And to
  conserve colours, gluons contain a mix of colours to do so. The Lie group
  structure enforces colour conservation here... Since these colour rotations can be different at
  different points in space and time, these symmetries can also be
  called a \textbf{local symmetry} or a \textbf{gauge symmetry}.
\item A \textbf{global symmetry} is a symmetry that leaves something the same
  across all space and time. 
\item A \textbf{external symmetry} is a symmetry involving spacetime
  coordinates. This includes symmetry under translation and Lorentz
  transformations. From these symmetries we get conserved structures (momentum,
  angular momentum, and energy). The Poincare group contains all these
  symmetries. 
\end{itemize}

In terms of particles, bosons carry forces, and these includes gluons (strong
force), photons (electromagnetic force), $Z^0, W^\pm$ carries the electroweak
force. It has also been hypothesised that the graviton (spin 2) carries gravity,
although it has never been observed. Also, for good symmetries, force carriers
should be massless, but the spontaneous symmetry breaking, through the Higgs
mechanism, can give force carriers mass (such as $W^\pm, Z^0$).

The lecturer provides a standard list of (elementary) fermions. I won't repeat
these.

Just as a note, in the standard model, every particle has a field, and
excitations of this field corresponds to ``instances'' of these particles.
[End of lecture 1]

In lecture 2, the lecturer reviews basic group theory. I won't repeat that here.
[End of lecture 2]

We continue looking at group properties, etc. Here are some definitions.

\begin{definition}
  The \textbf{inner automorphism} associated with $g \in G$ is $\phi_g(h) =
  ghg^{-1}$.
\end{definition}

We remark that treating elements of $G$ as automorphisms in this way we see that
$G / Z(G)$ is always a normal subgroup of $\text{Aut}(G)$.

\begin{definition}
  The \textbf{semi-direct product} of groups $H, G$, written $H \ltimes G$, has the
  product rule $(h, g)(h', g') = (hh', g\phi_h(g'))$, and inverse rule $(h,
  g)^{-1} = (h^{-1}, \phi_{h^{-1}}(g))$.
\end{definition}

here we can see that $H \equiv H \ltimes G / G$, and that $D_n \equiv
\mathbb{Z}_2 \ltimes \mathbb{Z}_n$ as expected.

\begin{definition}
  The \textbf{commutator subgroup} or \textbf{derived subgroup} of group $G$, denoted $[G, G]$,
  the group generated by the commutators of $G$. These are always normal, and a
  group is called \textbf{perfect} when it equals its own commutator subgroup.
\end{definition}

\section{Matrix Groups}

The lecturer describes the general linear group, orthogonal group, special
orthogonal group, unitary group, and special unitary group. He also describes
their dimensionality. In particular, using their properties, he shows that
$O(n)$ has $\frac{1}{2}n(n - 1)$ free parameters, and $SU(n)$ has $n^2 - 1$
parameters. He also mentions the famous fact that topologically $SU(2) \equiv
S^3$. [End of lecture 3]

Let's continue defining some special matrix groups that may be less well known.

\begin{definition}
  The \textbf{symplectic group} $S_p(2n, \mathbb{R})$ is a subset of the general
  linear group satisfying
  $$ M^T J M = J $$
  for
  $$ J =
  \begin{pmatrix}
    0 & -1 & 0 & 0 & \dots & 0 & 0 \\
    1 & 0 & 0 & 0 & \dots & 0 & 0 \\
    0 & 0 & 0 & -1 & \dots & 0 & 0 \\
    0 & 0 & 1 & 0 & \dots & 0 & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & 0 & \dots & 0 & -1 \\
    0 & 0 & 0 & 0 & \dots & 1 & 0
  \end{pmatrix}
  $$
\end{definition}

This group has dimension $n(2n - 1)$ when treated as a manifold, and the
determinant of every element in the group is 1 (applying the determinant to the
equation gives us $\pm1$, if you apply a generalised determinant for
anti-symmetric matrices called the Pfaffian you can see the sign as well). For
the sake of representations the most important thing about the symplectic
symplectic group is that unlike other groups we've seen, like $SO(n)$ or
$SU(n)$, the symplectic linear group is not compact.

\begin{definition}
  The pseudo-orthogonal group $O(n, m)$ is the subgroup of the general linear
  group satisfying
  $$ M^T g M = g $$
  for
  $$ g =
  \begin{pmatrix}
    I_n & \\
    & -I_m
  \end{pmatrix}
  $$
\end{definition}

Similarly we have $SO(n, m), U(n, m), SU(n, m)$. Also notice that $SO(1, 1) =
S_p(2, \mathbb{R})$.

\subsection{Representations}

A representation of a group is a homomorphism $\phi$ from that group to a space of
linear maps on some vector space $V$ on some field. For matrix groups, such as
those mentioned above, we 
also have the canonical \textbf{fundamental representation} that represents
matrices just as the matrices they are defined to be.

A representation is called \textbf{reducible} if we can find a linear subspace
so that the representation can be viewed purely as a representation on that
subspace (formally $\forall g \in G, u \in U, \phi(g) u \in U$ where $U \leq
V$). There are theorems in representation theory about the reducibility of
representations into irrudicible ones, such that we can always write

$$ V \equiv \bigoplus_r U_r $$

for irreducible $U_r$. A handy lemma in this field.

\begin{lemma}
  For irreducible representations $V, W$ of G,
  \begin{itemize}
  \item $Hom_G(V, W)$ ccontains only 0 or isomorphisms.
  \item if the field we are working on is algebraically closed, then
    $\text{dim} (Hom_G(V, W)) = 1$ 
  \end{itemize}
\end{lemma}

I know this has been worded in too abstract a manner, but it is more familiar
for me. Basically, two irreducible representations are either isomorphic,
meaning that they are the same up to similarity transformations (change of
bases), and there is only one similarity transformation up to multiplication by
a constant factor.

Despite the fact that any group has an infinite number of representations, a lot
of information can be summarised about them by calculating the
\textbf{character} $\chi(g) = tr(\phi(g))$, which is just the trace of every
matrix. Notice that since traces are unchanged by similarity transformations,
they firstly are the same for isomorphic/equivalent representations, and
secondly, are the same for conjugacy classes within the group, so they truly are
just summaries of the representation. Nonetheless, many important properties can
be deduced using them. For one, it is much easier to check if representations
are equivalent this way.

Finally, we consider the \textbf{tensor product} such that on $V \otimes W$
the product of two representations acts as

$$ (\phi \otimes \psi) (g) (v \otimes w) = \sum_{rs} \lambda_r \mu_s \phi(g)(v_r)
\otimes \psi(g)(w_s) = \phi(g)(v) \otimes \psi(g)(w) $$

where $v_r, w_s$ are the basis vectors of $V, W$ and $\lambda_r, \mu_s$ express
$v$, $w$ in these bases. [End of lecture 4]

A quick note is that one can calculate the tensor product on characters as

$$ \tr_{V_r \otimes V_s}(D^{R_r} (g) D^{R_s}(g)) = \tr_{V_r} (D^{R_r}(g))
\tr_{V_s} (D^{R_s}(g)) $$

Also, if $R_r \otimes R_s$ contains the singlet representation then we can
construct an inner product $\langle v, v' \rangle$ which is invariant under the
group.

\subsection{Symmetries and Quantum Mechanics}

Alright, now let's actually start using some of these mathematical ideas!

A (unitary) map $U$ is called a \textbf{symmetry} if $|\langle \phi | \psi
\rangle = |\langle U\phi, U\psi \rangle|^2$ for all $\phi, \psi$. Furthermore,
one can show that $U$ is always linear or anti-linear (\textbf{anti-linear}
means it is a linear map except it applies complex conjugation, so $Ua\ket{\psi}
= a^*U\ket{\psi}$). Furthermore, it can be shown that anti-linear maps are only
linear when we consider time reversal. Therefore, we will focus on linear maps.

One can also show that such maps, if they represent a symmetry group $G$ satisfy
the product rule

$$U(g) U(g') = e^{i\gamma(g, g')}U(gg')$$

where we usually just assume that $\gamma = 0$ leaving us with a homomorphism.
One can further show that it must always commute with the Hamiltonian if it is a
symmetry so

$UH = HU$

meaning that we can find a simultaneous eigenbasis for the two.

\section{Rotations $SO(3)$ and $SU(2)$}

An important group of symmetries are the group of rotations $SO(3)$ and $SU(2)$.
We will prove an important results on them, namely that

$$ SO(3) \equiv SU(2) / \mathbb{Z}_2 $$

To do so we will develop an important tool, namely Pauli matrices. But before we
start, let's review some standard properties of these groups. In $SO(3)$ we can
write a general rotation by $\theta$ about axis $n$ as

$$ R_{ij} = \cos(\theta) \delta{ij} + (1 - \cos(\theta)) n_i n_j - \sin(\theta)
\epsilon_{ijk} n_k $$

which corresponds infinitesimally to

$$ \delta x = \delta n \times x. $$

Now, let's start proving this isomorphism. First, the Pauli matrices are:

$$
\sigma_1 =
\begin{pmatrix}
  & 1 \\
  1 & \\
\end{pmatrix},
\sigma_2 =
\begin{pmatrix}
  & -i \\
  i & \\
\end{pmatrix},
\sigma_3 =
\begin{pmatrix}
  1 & \\
  & -1 \\
\end{pmatrix}
$$

which satisfy the properties

$$ \sigma_i \sigma_j = \delta_{ij}I + \epsilon_{ijk} \sigma_k $$

and

$$ \tr(\sigma_i \sigma_j) = 2 \delta_{ij} $$

In other words, what exactly are the Pauli matrices? The Pauli matrices form an
orthogonal basis (up to a factor of 2) when we treat the space of Hermitian traceless
$2\times 2$ matrices as a real vector space. Hermitian means the diagonal
entries must be real, and traceless means the top left and top right entries
must add to 0. This is generated by $\sigma_3$. Meanwhile, $\sigma_1, \sigma_2$
handle the off-diagonal basis. Why are they orthogonal? The natural inner
product to use on matrix spaces is to rearrange the matrices into long vectors,
and just to apply the dot-product to these vectors, and that can exactly be
written as

$$ A \cdot B = \tr{AB} = A_{ij} B_{ij} $$

Under this product, the property $ \tr(\sigma_i \sigma_j) = 2 \delta{ij}$ means
these matrices truly are orthogonal. Now, we can easily form a complete basis
for the space of $2 \times 2$ Hermitian matrices by adding the identity $I$.
When doing so, we find that any matrix $A \in SU(2)$ can be written in this
basis as

$$ A = \frac{1}{2} \tr(A) I + \frac{1}{2} \tr(\sigma_i A) \sigma_i $$

Note in particular, that $\tr(A) = \tr(AI)$, so that if we define $\sigma_0 = I$
we get

$$ A = \frac{1}{2} \tr{\sigma_\mu A} \sigma_\mu, \,\, \mu = 0, 1, 2, 3 $$

which is really just writing it in a new orthogonal basis according to a certain
dot product. [End of lecture 5]

So what is the isomorphism we are going to use. Consider the map

$$ x \mapsto x_i \sigma_i $$

which has inverse

$$ x_i \sigma_i \mapsto \frac{1}{2} \tr(\sigma_j x_i \sigma_i) $$

and satisfies $(x_i \sigma_i)^2 = x^2 I$, and $x_i \sigma_i$ has eigenvalues
$\pm\sqrt{x^2}$, leaving determinant $\det(x_i \sigma_i) = -x^2$.

Now, for any $A \in SU(2)$ let $x \mapsto x'$ via

$$ x'_i \sigma_i = Ax_i \sigma_i A^\dagger $$

which has $x'^2 = -det{x'_i \sigma_i} = \det{x_i \sigma_i} = x^2$ since $A$ is
unitary. That means this map preserves length and so sits in $O(3)$. How do we
show it is a rotation, not a reflection? We notice that the determinant as a map
is continuous, and as $A \to I$, $R_A \to I$ as well, so we must get $\det{R_A}
= 1$. So if we assume that $SU(2)$ has only a single connected component, we see
that we map entirely onto $SO(3)$.

Now, we'd like to invert this map as well, and a first step for us would be to
see if we can write this map down explicitly. To do so, we notice that

$$ (Rx)_i \sigma_i = \sigma_i R_{ij} x_j = x'_i \sigma_i = Ax_j \sigma_j
A^\dagger $$

which is true for any $x$, meaning that

$$ \sigma_i R_{ij} = A \sigma_j A^\dagger $$

Here, the LHS is just a decomposition of a matrix into a sum of Pauli, matrices,
so taking a dot product (trace) with the appropriate matrices to undo the
composition we get

$$ R_{ij} = \frac{1}{2} \tr(\sigma_i A \sigma_j A^\dagger) $$

which gives us one direction for our map. To reverse this, we notice that
$\sigma_i \sigma_j \sigma_i = -\sigma_j$, and $\sigma_i^2 = I$, meaning that
$\sigma_\mu \sigma_\nu \sigma_\mu = 4\delta_{0 \nu} I$. 

$$ \sigma_\mu A^\dagger \sigma_\mu = 2 \tr(A) I \implies \sigma_i A^\dagger
\sigma_i = 2 \tr(A) - \sigma_0 A^\dagger \sigma_0 $$

(the implication refers to an equation in lectures).

Applying this to our formula for $R_{ij}$ we gets

$$ R_{jj} = \frac{1}{2} \tr(\sigma_j A \sigma_j A^\dagger) = \frac{1}{2}
\tr(\sigma_\mu A \sigma_\mu A^\dagger - \sigma_0 A \sigma_0 A^\dagger) =
\tr(\tr(A) A^\dagger) - \frac{1}{2} \tr(AA^\dagger) = |\tr(A)|^2 - 1 $$

Furthermore, we get

\begin{align*}
\sigma_i R_{ij} \sigma_j
&= \frac{1}{2} \sigma_i \tr(\sigma_i A \sigma_j A^\dagger) \sigma_j \\
&= \frac{1}{8} \sigma_i \tr(\sigma_i \tr(\sigma_\mu A) \sigma_\mu \sigma_j \tr(\sigma_\nu A^\dagger) \sigma_\nu) \sigma_j \\
&= \dots \\
&= 2\tr(A)A - I
\end{align*}

(by taking out the traces inside, and analysing case-by-case one can find nice
expressions for the trace of products of Pauli matrices). These together leave

$$ A = \pm \frac{I + \sigma_i R_{ij} \sigma_j}{2\sqrt{1 + R_{jj}}} $$

which completes our isomorphism.

We also find we can express these relations efficiently infinitesimally:

$$ R_{ij} = \delta_{ij} - \delta \theta \epsilon_{ijk} n_k $$

$$ A = I - i \delta \theta n_k \sigma_k / 2 $$

since $\det(I + X) = 1 + \tr(X)$ meaning that in order to keep $\det = 1$ to
first order we need $\tr(X) = 0$ above. Now, we can recover an arbitrary
rotation by exponentiation, and then see that

$$ A(\theta, n) = e^{-\frac{i}{2} n_k \sigma_k \theta} = 2 \cos(\theta / 2) - i
  n_k \sigma_k \sin(\theta / 2) $$

This makes it clearer where the $\mathbb{Z}_2$ symmetry comes from.

\subsection{Infinitesimal Generators and Rotations}

Now for rotations $R_1 = R(\delta \theta_1, n_1), R_2 = R(\delta \theta_2, n_2)$
we find that commutator $R_c = R_2^{-1}R_1^{-1}R_2R_1$ is given infinitesimally
by

$$ x \mapsto x + \delta \theta_1 \theta_2 (n_2 \times (n_1 \times x) - n_1
\times (n_2 \times x)) = x + \delta \theta_1 \delta \theta_2 $$

In general, in quantum mechanics we assume any unitary operation has a
generator, such as for rotations

$$ U(R(\delta \theta, n)) = I - i \delta \theta n \cdot J $$

for generators $J$ of rotations about each axis. Note that in general, $U$
unitary means that generators $J_i$ are all Hermitian. As such we see that

$$ U(R_c) = I - i \delta \theta_1 \delta \theta_2 (n_2 \times n_1) \cdot J = 1 -
\delta \theta_1 \delta \theta_2 [n_2 \cdot J, n_1 \cdot J_2] $$

which gives us the famous commutation relation

$$ [J_i, J_j] = i \epsilon_{ijk} J_k $$

Similarly, for a function of space, where we use $L$ instead of $J$ as a
generator of rotations we gets

$$ f \mapsto (1 - i \delta \theta \cdot n L) f(x) $$

which gives rise to thse same commutation relations as we had before. We can
then recover finite rotations by exponentiating as expected. Now, we also see
that rotations correspond to conservation of angular momentum since they
conserve the Hamiltonian.

\subsection{Representations of the Angular Momentum Commutation Relations}

For convenience when dealing with the angular momentum operator (generator of
rotations) we we define $J_{\pm} = J_1 \pm iJ_2$ (which become our ladder
operators), and gives rise to the commutation relations

$$ [J_3, J_\pm] = \pm J_\pm $$

$$ [J_+, J_-] = 2 J_3 $$

and of course, $J_3^* = J_3, J_+^* = J_-$. We then pick our basis, which leads
us to

$$ J_3 \ket{m} = m \ket{m} $$

$$ J_+ \ket{m} = \lambda_m \ket{m \pm 1} $$

$$ J_- \ket{m} = \ket{m - 1} $$

or 0 (sometimes $J_\pm$ annihilates a state). Using commutation relations we can
then deduce that

$$ \lambda_m = j(j + 1) - m(m + 1). $$

(since we see that $\lambda_{m - 1} - \lambda_m = 2m$)

[End of lecture 6]

We notice that this means that for large $m$ $\lambda_m$ becomes negative,
however, $J_\pm J_\mp$ is non-negative definite meaning that all eigenvalues
must be non-negative. Consequently there must exist $m_\pm$ such that $J_\pm
\ket{m_\pm} = 0 = (j \mp m_\pm)(j \pm m_\pm + 1)$. This means that $m_\pm = \pm
j$ for $j \in \{0, 1/2, 1, 3/2, \dots\}$. These states form an orthogonal basis
for a representation $\mathcal{V}_j$ of dimension $2j + 1$.

\subsubsection{The $\ket{j m}$ Basis}

[Explain why we need these representations?]

We consequently get an orthogonal $\ket{jm}$ basis such that $J_3 \ket{j\,m} = m
\ket{j m}$. Here we can define a \textbf{highest weight state} to be $\ket{j\,j}$.
Now

$$ J_\pm \ket{j\,m} = \sqrt{(j - m)(j + m + 1)} \ket{j\, m \pm 1} $$

means that we can define states as

$$ (J_-)^n \ket{j \, j} = \sqrt{\frac{n! (2j)!}{(2j - m)!}} \ket{j \, j - n} $$

That is one way of defining $j$, but of course, we also have the more classical
picture using the Casimir operator

$$ J^2 = J_i J_i = J_+ J_- + J_3^2 - J_3 $$

If we do that we see that $J\ket{j\, j} = j(j + 1) \ket{j\, j}$. But we notice
that $[J^2, J_-] = 0$ meaning that in general

$$ J^2 \ket{j \, m} = j(j + 1) \ket{j\, m} $$

meaning that indeed, $J^2$ can be considered to correspond to the total angular
momentum. 

Now that we've sorted out the overall algebraic properties that our
representation should have, let's figure out what the actual representations of
this space are. Here we see that we get

\begin{align*}
J^{(j)}_{3 \, m'm} &= m \delta_{m'm} \\
J^{(j)}_{\pm \, m'm} = \sqrt{(j \mp m)(j \pm m + 1)} \delta_{m'm \pm 1}
\end{align*}

which completes our representation for rotation generators $J$ in the basis such
that our matrices are $\bra{j \, m'} J_i \ket{j \, m}$. We'd like to define a
similar representation for finite rotations $D(R)$ such that infinitesimally

$$ D^{(j)}(R(\delta n, n)) = I - i \delta \theta n \cdot J^{(j)} $$

To get the finite version, we consider the Euler angles for an arbitrary
rotation such that

$$ R = R(\phi, e_3) R(\theta, e_2) R(\psi, e_3) $$

giving representation

$$ U(R) = e^{-i \phi J_3} e^{-i\theta J_2} e^{-i\psi J_3} $$

meaning that

$$ D^{(j)}_{m'm} = e^{-i m' \phi - i m \psi} d^{(j)}_{m'm}(\theta) $$

for

$$ d^{(j)}_{m'm}(\theta) = \bra{j \, m'} e^{-i \theta J_2} \ket{j \, m} $$

A few special cases can be worked out as $d^{(j)}_{m'm}(\pi) = (-1)^{j - m}
\delta_{m', -m}$ and $d^{(j)}_{m'm} (2\pi) = (-1)^{2j} \delta_{m'm}$, and in
fact in general

$$ D^{(j)}(R(2\pi, n)) = (-1)^{2j} I $$

where we note crucially that $j \in \{0, 1/2, 1, 3/2, \dots\}$. We also find the
general result that

$$ d^{(j)}_{m'm} (\theta) = (-1)^{m' - m} d^{(j)}_{-m', -m}(\theta) = (-1)^{m' -
  m} d^{(j)}_{m'm} (-\theta) = (-1)^{m' - m} d^{(j)}_{mm'}(\theta) $$

implying a degree of symmetry at least.

Furthermore, we see that in the special case $j=1/2$ we get $J_i = \sigma_i$ the
Pauli matrices, and

$$ d^{(1/2)}(\theta) =
\begin{pmatrix}
  \cos(\theta / 2) & -\sin(\theta / 2) \\
  \sin(\theta / 2) & \cos(\theta / 2)
\end{pmatrix}
$$

and we can generate the character of a rotation by angle $\theta$ to in general
be (independent of the axis of rotation)

$$ \chi_j(\theta) = \frac{\sin((j + 1/2) \theta)}{\sin(\theta / 2)} $$

\subsubsection{Tensor Products and Angular Momentum Addition}

Now a somewhat well known decomposition is what happens when you add to
representations together and look at the new representation structure. In
particular, how do we look at the tensor product $\mathcal{V}_{j_1} \otimes
\mathcal{V}_{j_2}$ with basis $\ket{j_1 \, m_1} \ket{j_2 \, m_2}$. Then we can
extend our generators to the new space via 

$$ J_1 = J_1 \otimes I_2 $$
$$ J_2 = I_1 \otimes J_2 $$

[End of lecture 7]

We then form a new basis $\ket{J\, M}$ which we define to satisfy

$$ J_3 \ket{J \, M} = M \ket{J \, M} $$
$$ J^2 \ket{J \, M} = J(J + 1) \ket{J \, M} $$

but of course, we also have the natural basis inherited from the definition of
the space: $\ket{j_1 \, m_1 \, j_2 \, m_2}$, so how do these relate? We define
the \textbf{Clebsch-Gordan} coefficients to be precisely to coefficients
expressing $\ket{J\, M}$ in terms of $\ket{j_1 \, m_1 \, j_2 \, m_2}$, where we
note in particular that in order to make $J_3$ values match Clebsch-Gordan
coefficients are only nonzero when $m_1 + m_2$.

Since we also inherit the raising and lowering operators, we might hope that we
can, just as with a pure state $\mathcal{V}_j$, find a highest weight state
satisfying 

$$ J_3 \ket{J\, J} = J \ket{J \, J}$$
$$ J_+ \ket{J \, J} = 0$$

By thinking a little, we can see that the following definitely works

$$ \ket{j_1 + j_2 \, j_1 + j_2} = ket{j_1\, j_1} \ket{j_2\, j_2} $$

We can then use $J_-$ to find the rest of the coefficients. Note that we could
also have started with a state annihilated by $J_-$ instead and then use $J_+$
instead, although it really comes down to the same (but perhaps this implies
some symmetry in the Clebsch-Gordan coefficients?). 

Now, if we define $\mathcal{V}^{(M)}$ to be eigenspace of $J_3$ with eigenvalue
$M$ then, by counting on the original space, we see that

$$ \text{dim}(\mathcal{V}^{(M)}) = 
\begin{cases}
j_1 + j_2 - M + 1 & M \geq j_1 - j_2 \\
2j_2 + 1 & M \leq j_1 - j_2
\end{cases}
$$

where we've assumed without loss of generality that $j_1 \geq j_2$. Now, from
the above we know how we ``travel'' inside of $\mathcal{V}^{(M)}$ for fixed $M$,
but we have yet to see how we travel between different values of $J$. We do not
have a particularly efficient way of doing so: we only see that by exploiting
dimensions efficiently we see that $\ket{j_1 + j_2 \, j_1 + j_2 - 1}$ is
orthogonal to $\ket{j_1 + j_2 - 1 \, j_1 + j_2 - 1}$, and similarly, we
gradually can find Clebsch-Gordan coefficients for $J < j_1 + j_2$.

By further considering the original spaces, we see that $J \in \{j_1 + j_2,
\dots, \abs{j_1 - j_2}\}$. Using $\text{dim}(\mathcal{V}^{(J)}) = 2J + 1$ we
can check we do get the correct dimension of our space $(2j_1 + 1)(2j_2 + 2)$.
[The lecturer mentions some straightforward orthogonality relationships, and
gives an example for $j_1 = 1, j_2 = 1/2$.]

\subsection{$SO(3)$ tensors}

When studying how a certain system $S$ behaves there are two questions one might
wish to consider. Firstly, how does $S$ behave as the input is varied? This
generally is the topic of calculus, and differential equations, etc. Secondly,
under what transformations does $S$ stay the same? This is generally the topic
of group theory and the like. This course certainly focuses more on the second
question, and a concept that illustrates this perhaps the best is that of
tensors. Tensors, after all, essentially are things that are left invariant
under certain group transformations. The numerical representations of these
states change under these transformations, but tensors essentially define
themselves as objects whose ``true'' value is unchanged under these
transformations.

The simplest type of tensors for us to consider are tensors under rotations, so
tensors under $SO(3)$. Here, we can see that the matrix multiplying indices are
the representation of the group, while the tensors themselves inhabit the linear
space that that group is acting on. As such tensors are just representations of
the group that determines their transformation rule, and as such we might ask
what spaces of tensors are irreducible representations? If we denote the space
of rank-$n$ tensors as $\mathcal{V}^{\otimes n}$, then we see in particular that
isotropic tensors form irreducible representations. As such one can deduce that
all rank 2 tensors split into the following irreducible representations: For any
rank-2 tensors $T_{ij}$

$$ T_{ij} = S_{ij} + \epsilon_{ijk} v_k + \frac{1}{3} \delta_{ij} T_kk $$

where $S_{ij}$ is a symmetric, traceless tensor, $v_k$ is a rank-1 tensor
describing the anti-symmetric part of $T_{ij}$ and the last term captures the
trace.

To generalise we'd like to be able to find an easy test of irreducibility on
these spaces. Here we see that for candidate space $S_{i_1 \dots i_n}$ we see
that $\delta_{i_k i_{k'}} S_{i_1 \dots i_n} = 0$ always if $S_{i_1 \dots i_n}$
is irreducible (otherwise the space of such nonzero tensors would generate a
subspace). As such we one such irreducible space is totally symmetric and
traceless. 

Finally, we remark that summing over all indices forms an invariant inner
product on the space. [End of lecture 8] 

\end{document}